{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Tumor Detection\n",
    "## Single Task Validation - Building multi-lable binary Classifier\n",
    "Description\n",
    "This dataset was originally created by Yousef Ghanem. To see the current project, which may have been updated since this version, please go here: https://universe.roboflow.com/yousef-ghanem-jzj4y/brain-tumor-detection-fpf1f.\n",
    "\n",
    "This dataset is part of RF100, an Intel-sponsored initiative to create a new object detection benchmark for model generalizability.\n",
    "\n",
    "Access the RF100 Github repo: https://github.com/roboflow-ai/roboflow-100-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/brain-tumor-detection'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Go to project root folder\n",
    "import os\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 08:25:09.354070: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741854309.361904   76591 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741854309.364250   76591 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1741854309.372938   76591 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1741854309.372952   76591 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1741854309.372953   76591 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1741854309.372955   76591 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-13 08:25:09.376465: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')], '2.19.0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_gpu = tf.config.list_physical_devices('GPU')\n",
    "if not found_gpu:\n",
    "    raise Exception(\"No GPU found\")\n",
    "found_gpu, tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_handler.data_loader import DataLoader\n",
    "from src.data_handler.annotation_processor import AnnotationProcessor\n",
    "from src.data_handler.preprocessor import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto reload dotenv \n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "# auto reload libs\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/brain-tumor-2/train/\n"
     ]
    }
   ],
   "source": [
    "from hydra import initialize, compose\n",
    "\n",
    "# https://gist.github.com/bdsaglam/586704a98336a0cf0a65a6e7c247d248\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../conf\"):\n",
    "    cfg = compose(config_name=\"config\")\n",
    "    print(cfg.DATASET_DIRS.TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TRAIN_DIR': '${DATASET.DATASET_DIR}/${DATASET.DATASET_NAME}/train/', 'VALIDATION_DIR': '${DATASET.DATASET_DIR}/${DATASET.DATASET_NAME}/valid', 'TEST_DIR': '${DATASET.DATASET_DIR}/${DATASET.DATASET_NAME}/test'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.DATASET_DIRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIRS = Path(cfg.DATASET.DATASET_DIR)\n",
    "TRAIN_DIR = Path(cfg.DATASET_DIRS.TRAIN_DIR)\n",
    "VALIDATION_DIR = Path(cfg.DATASET_DIRS.VALIDATION_DIR)\n",
    "TEST_DIR = Path(cfg.DATASET_DIRS.TEST_DIR)\n",
    "\n",
    "\n",
    "IMG_SIZE = cfg.TRAIN.IMG_SIZE\n",
    "BATCH_SIZE = cfg.TRAIN.BATCH_SIZE\n",
    "LOG_DIR = cfg.OUTPUTS.LOG_DIR\n",
    "CHECK_POINT_DIR = Path(cfg.OUTPUTS.CHECKPOINT_PATH)\n",
    "CLASS_NAME = [\n",
    "    'label0',\n",
    "    'label1',\n",
    "    'label2'\n",
    "]\n",
    "class_map = {k: v for k, v in enumerate(CLASS_NAME)}\n",
    "\n",
    "NUM_EPOCHS = cfg.TRAIN.NUM_EPOCHS\n",
    "LEARNING_RATE = cfg.TRAIN.LEARNING_RATE\n",
    "\n",
    "NUM_CLASSES = len(CLASS_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Download from Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_DIR.exists():\n",
    "    from roboflow import Roboflow\n",
    "    rf = Roboflow()\n",
    "    project = rf.workspace(\"roboflow-100\").project(\"brain-tumor-m2pbp\")\n",
    "    version = project.version(2)\n",
    "    dataset = version.download(\"tensorflow\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images from directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6930, 6930, 6930)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_train_dataset = AnnotationProcessor(annotation_file= str(TRAIN_DIR/'_annotations.csv'))\n",
    "_class_map = {v: k for k, v in enumerate(CLASS_NAME)}\n",
    "train_images, train_class_ids, train_bboxes  = prepare_train_dataset.process_annotations(image_dir=TRAIN_DIR, class_id_map=_class_map)\n",
    "\n",
    "len(train_images), len(train_class_ids), len(train_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('datasets/brain-tumor-2/train/volume_337_slice_89_jpg.rf.63cc21fc850bfb89383c90a49ece9826.jpg',\n",
       " [0, 1, 2],\n",
       " array([[0.57916667, 0.33333333, 0.75833333, 0.425     ],\n",
       "        [0.5375    , 0.275     , 0.82916667, 0.5125    ],\n",
       "        [0.57083333, 0.32083333, 0.76666667, 0.45416667]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0],train_class_ids[0], train_bboxes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_images, train_class_ids, train_bboxes).load_train_dataset()\n",
    "train_ds = Preprocessor(train_dl).preprocess()\n",
    "train_ds = train_ds.batch(BATCH_SIZE)\\\n",
    "                .prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3) (32, 3, 4)\n",
      "(32, 240, 240, 3)\n",
      "-123.68 131.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 08:32:44.966302: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for batch in train_ds.take(1):\n",
    "    image, (cls, bbx) = batch\n",
    "    print(cls.shape,bbx.shape)\n",
    "    print(image.shape)\n",
    "    print(image[1].numpy().min(), image[1].numpy().max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1980, 1980, 1980)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_valid_dataset = AnnotationProcessor(annotation_file= str(VALIDATION_DIR/'_annotations.csv'))\n",
    "\n",
    "valid_image_paths, valid_class_ids, valid_bboxes  = prepare_valid_dataset.process_annotations(image_dir=VALIDATION_DIR, class_id_map=_class_map)\n",
    "len(valid_image_paths), len(valid_class_ids), len(valid_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dl = DataLoader(valid_image_paths, valid_class_ids, valid_bboxes).load_train_dataset()\n",
    "valid_dl = Preprocessor(valid_dl).preprocess()\n",
    "valid_dl = valid_dl.batch(BATCH_SIZE)\\\n",
    "                .prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3) (32, 3, 4)\n",
      "(32, 240, 240, 3)\n",
      "-123.68 133.22101\n"
     ]
    }
   ],
   "source": [
    "for batch in valid_dl.take(1):\n",
    "    image, (cls, bbx) = batch\n",
    "    print(cls.shape,bbx.shape)\n",
    "    print(image.shape)\n",
    "    print(image[1].numpy().min(), image[1].numpy().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ResNet50 Model Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall'),\n",
    "    tf.keras.metrics.AUC(name='AUC', multi_label=True), \n",
    "    tf.keras.metrics.F1Score(name='f1_score',average='weighted'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define  Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "to_monitor = 'val_loss'\n",
    "mode = 'min'\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, \n",
    "                                            patience=2, \n",
    "                                            monitor=to_monitor,\n",
    "                                            mode=mode,\n",
    "                                            min_lr=1e-6,\n",
    "                                            verbose=1),\n",
    "\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(str(CHECK_POINT_DIR), \"ckpt_{epoch}.keras\") ,\n",
    "                                        save_weights_only=False,\n",
    "                                        save_best_only=True,\n",
    "                                        monitor=to_monitor,\n",
    "                                        mode=mode,\n",
    "                                        verbose=1),\n",
    "                                        \n",
    "    tf.keras.callbacks.EarlyStopping(monitor=to_monitor, \n",
    "                                    patience=10,\n",
    "                                    mode=mode, \n",
    "                                    restore_best_weights=True),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, use_ema=True)\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE, momentum=0.9, clipvalue=1.0)\n",
    "\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9, clipvalue=1.0)\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building and Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def iou_loss(y_true, y_pred):  # Assuming y_true and y_pred are (batch_size, 4)\n",
    "    # y_true = y_true[0]\n",
    "    # y_pred = tf.reshape(y_pred, ( 3, 4))\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32) # Cast to float32\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32) # Cast to float32\n",
    "\n",
    "    x_true = y_true[..., 0]\n",
    "    y_true_ = y_true[..., 1]\n",
    "    x_max_true = y_true[..., 2]\n",
    "    y_max_true = y_true[..., 3]\n",
    "\n",
    "    x_pred = y_pred[..., 0]\n",
    "    y_pred_ = y_pred[..., 1]\n",
    "    x_max_pred = y_pred[..., 2]\n",
    "    y_max_pred = y_pred[..., 3]\n",
    "\n",
    "    area_true = (x_max_true - x_true) * (y_max_true - y_true_)\n",
    "    area_pred = (x_max_pred - x_pred) * (y_max_pred - y_pred_)\n",
    "\n",
    "    x_intersect = tf.maximum(x_true, x_pred)\n",
    "    y_intersect = tf.maximum(y_true_, y_pred_)\n",
    "    x_max_intersect = tf.minimum(x_max_true, x_max_pred)\n",
    "    y_max_intersect = tf.minimum(y_max_true, y_max_pred)\n",
    "\n",
    "    area_intersect = tf.maximum(0.0, x_max_intersect - x_intersect) * tf.maximum(0.0, y_max_intersect - y_intersect) # avoid negative values\n",
    "    iou = area_intersect / (area_true + area_pred - area_intersect + 1e-7)  # Add small epsilon for numerical stability\n",
    "    return 1.0 - iou  # We want to *minimize* the loss\n",
    "\n",
    "def iou_metric(y_true, y_pred):  # No negation for metric\n",
    "    # y_true = y_true[0]\n",
    "    # y_pred = tf.reshape(y_pred, (3, 4))\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32) # Cast to float32\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32) # Cast to float32\n",
    "\n",
    "    x_true = y_true[..., 0]\n",
    "    y_true_ = y_true[..., 1]\n",
    "    x_max_true = y_true[..., 2]\n",
    "    y_max_true = y_true[..., 3]\n",
    "\n",
    "    x_pred = y_pred[..., 0]\n",
    "    y_pred_ = y_pred[..., 1]\n",
    "    x_max_pred = y_pred[..., 2]\n",
    "    y_max_pred = y_pred[..., 3]\n",
    "\n",
    "    area_true = (x_max_true - x_true) * (y_max_true - y_true_)\n",
    "    area_pred = (x_max_pred - x_pred) * (y_max_pred - y_pred_)\n",
    "\n",
    "    x_intersect = tf.maximum(x_true, x_pred)\n",
    "    y_intersect = tf.maximum(y_true_, y_pred_)\n",
    "    x_max_intersect = tf.minimum(x_max_true, x_max_pred)\n",
    "    y_max_intersect = tf.minimum(y_max_true, y_max_pred)\n",
    "\n",
    "    area_intersect = tf.maximum(0.0, x_max_intersect - x_intersect) * tf.maximum(0.0, y_max_intersect - y_intersect) # avoid negative values\n",
    "    iou = area_intersect / (area_true + area_pred - area_intersect + 1e-7)  # Add small epsilon for numerical stability\n",
    "    return iou  # Return IoU directly for metric\n",
    "\n",
    "def bounding_box_loss(y_true, y_pred):\n",
    "    # Reshape the output to (batch_size, 3, 4)\n",
    "    y_pred = tf.reshape(y_pred, (-1, 3, 4))\n",
    "    #y_true should be of shape (batch_size, 3, 4)\n",
    "    loss = 0\n",
    "    for tr_bb, pr_bb in zip(y_true, y_pred):\n",
    "        loss += tf.keras.losses.MeanSquaredError()(tr_bb[0, :], pr_bb[0,:])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = final_model()\n",
    "# model.compile(\n",
    "#     optimizer=optimizer,\n",
    "#     loss={'classification': 'binary_crossentropy', 'bounding_box': iou_loss},\n",
    "#     metrics={'classification': METRICS, 'bounding_box': iou_metric})  # Use IoU metric\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss={'classification': tf.keras.losses.BinaryCrossentropy(from_logits=False), 'bounding_box': tf.keras.losses.MeanSquaredError()},\n",
    "    metrics={'classification': METRICS, 'bounding_box': 'mse'})  # Use IoU metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(\"/brain-tumor-resnet50\")\n",
    "mlflow.tensorflow.autolog(log_models=True, log_datasets=False)\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[callbacks],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Datasets setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prepare_test_dataset = AnnotationProcessor(annotation_file= str(TEST_DIR/'_annotations.csv'))\n",
    "_class_map = {v: k for k, v in enumerate(CLASS_NAME)}\n",
    "test_image_paths, test_class_ids, test_bboxes = prepare_test_dataset.process_annotations(image_dir=TEST_DIR, class_id_map=_class_map)\n",
    "\n",
    "len(test_image_paths), len(test_class_ids), len(test_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_padded_class_ids, test_padded_bbx = pad_cls_id_bbx(test_class_ids, test_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datasets = tf.data.Dataset.from_tensor_slices((test_image_paths,\n",
    "                                               test_padded_class_ids,\n",
    "                                               test_padded_bbx))\n",
    "\n",
    "test_ds = test_datasets.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.repeat()\\\n",
    "                .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                . map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                .prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_ds, return_dict=True, steps=1)\n",
    "print(\"Testing accuracy: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metric_name, title, ylim=1):\n",
    "  plt.title(title)\n",
    "  plt.ylim(0,ylim)\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.legend([metric_name, 'val_' + metric_name])\n",
    "  plt.plot(history.history[metric_name],color='blue',label=metric_name)\n",
    "  plt.plot(history.history['val_' + metric_name],color='green',label='val_' + metric_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(\"loss\", \"val_loss\", ylim=0.2)\n",
    "plot_metrics(\"classification_f1_score\", \"classification_f1_score\", ylim=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(\"classification_loss\", \"Classification Loss\")\n",
    "plot_metrics(\"bounding_box_loss\", \"Bounding Box Loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
