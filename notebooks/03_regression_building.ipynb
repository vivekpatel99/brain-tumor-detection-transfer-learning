{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Tumor Detection\n",
    "## Single Task Validation - Building multi-lable binary Classifier\n",
    "Description\n",
    "This dataset was originally created by Yousef Ghanem. To see the current project, which may have been updated since this version, please go here: https://universe.roboflow.com/yousef-ghanem-jzj4y/brain-tumor-detection-fpf1f.\n",
    "\n",
    "This dataset is part of RF100, an Intel-sponsored initiative to create a new object detection benchmark for model generalizability.\n",
    "\n",
    "Access the RF100 Github repo: https://github.com/roboflow-ai/roboflow-100-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to project root folder\n",
    "import os\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_gpu = tf.config.list_physical_devices('GPU')\n",
    "if not found_gpu:\n",
    "    raise Exception(\"No GPU found\")\n",
    "found_gpu, tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_handler.data_loader import DataLoader\n",
    "from src.data_handler.annotation_processor import AnnotationProcessor\n",
    "from src.data_handler.preprocessor import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto reload dotenv \n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "# auto reload libs\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import initialize, compose\n",
    "\n",
    "# https://gist.github.com/bdsaglam/586704a98336a0cf0a65a6e7c247d248\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../conf\"):\n",
    "    cfg = compose(config_name=\"config\")\n",
    "    print(cfg.DATASET_DIRS.TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.DATASET_DIRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIRS = Path(cfg.DATASET.DATASET_DIR)\n",
    "TRAIN_DIR = Path(cfg.DATASET_DIRS.TRAIN_DIR)\n",
    "VALIDATION_DIR = Path(cfg.DATASET_DIRS.VALIDATION_DIR)\n",
    "TEST_DIR = Path(cfg.DATASET_DIRS.TEST_DIR)\n",
    "\n",
    "\n",
    "IMG_SIZE = cfg.TRAIN.IMG_SIZE\n",
    "BATCH_SIZE = cfg.TRAIN.BATCH_SIZE\n",
    "LOG_DIR = cfg.OUTPUTS.LOG_DIR\n",
    "CHECK_POINT_DIR = Path(cfg.OUTPUTS.CHECKPOINT_PATH)\n",
    "CLASS_NAME = [\n",
    "    'label0',\n",
    "    'label1',\n",
    "    'label2'\n",
    "]\n",
    "class_map = {k: v for k, v in enumerate(CLASS_NAME)}\n",
    "\n",
    "NUM_EPOCHS = cfg.TRAIN.NUM_EPOCHS\n",
    "LEARNING_RATE = cfg.TRAIN.LEARNING_RATE\n",
    "\n",
    "NUM_CLASSES = len(CLASS_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Download from Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_DIR.exists():\n",
    "    from roboflow import Roboflow\n",
    "    rf = Roboflow()\n",
    "    project = rf.workspace(\"roboflow-100\").project(\"brain-tumor-m2pbp\")\n",
    "    version = project.version(2)\n",
    "    dataset = version.download(\"tensorflow\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images from directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_train_dataset = AnnotationProcessor(annotation_file= str(TRAIN_DIR/'_annotations.csv'))\n",
    "_class_map = {v: k for k, v in enumerate(CLASS_NAME)}\n",
    "train_images, train_class_ids, train_bboxes  = prepare_train_dataset.process_annotations(image_dir=TRAIN_DIR, class_id_map=_class_map)\n",
    "\n",
    "len(train_images), len(train_class_ids), len(train_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images[0],train_class_ids[0], train_bboxes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, img_list:list[str], cls_id_list:list[list], bbx_list:list[list], num_classes=3):\n",
    "        self.img_list = img_list\n",
    "        self.cls_id_list = cls_id_list\n",
    "        self.bbx_list = bbx_list\n",
    "        self.num_classes = num_classes\n",
    "        self.data_augmentation = tf.keras.Sequential([\n",
    "            layers.RandomBrightness(0.1),\n",
    "            layers.RandomContrast(0.1),\n",
    "            layers.RandomSaturation(0.1),\n",
    "            layers.RandomHue(0.1)\n",
    "        ])\n",
    "\n",
    "    \n",
    "    def load_image(self, image_path) -> tf.Tensor:\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.cast(image, tf.float32) \n",
    "        return image\n",
    "\n",
    "    def load_dataset(self, image, class_ids, bbox):\n",
    "        tf_image = self.load_image(image)\n",
    "        # multi_hot = tf.reduce_max(tf.one_hot(tf.cast(class_ids, tf.int32), self.num_classes), axis=0 )  # Shape: (NUM_CLASSES,)\n",
    "        return  tf_image, bbox  #(multi_hot, bbox)\n",
    "        # return  tf_image,  {'classes': multi_hot, 'boxes': tf.cast(bbox, tf.float32)}\n",
    "    \n",
    "    def _common_loader(self)->tf.data.Dataset:\n",
    "        padded_class_ids, padded_bbx = self.pad_cls_id_bbx()\n",
    "        datasets = tf.data.Dataset.from_tensor_slices((self.img_list, padded_class_ids, padded_bbx))\n",
    "        ds = datasets.map(self.load_dataset, num_parallel_calls=tf.data.AUTOTUNE) \n",
    "        return ds\n",
    "    \n",
    "    def load_train_dataset(self)->tf.data.Dataset:\n",
    "        ds = self._common_loader()\n",
    "        ds =  ds.map(lambda x, y: (self.data_augmentation(x),y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        return ds.shuffle(buffer_size=ds.cardinality().numpy())\n",
    "\n",
    "    def load_val_dataset(self) ->tf.data.Dataset:\n",
    "        ds = self._common_loader()\n",
    "        return ds\n",
    "\n",
    "    def pad_cls_id_bbx(self):\n",
    "        \"\"\"\n",
    "        Pads class id and bounding box lists to the length of the longest in the batch.\n",
    "        \n",
    "        Args:\n",
    "            class_id_list (list): List of class ids.\n",
    "            bbox_list (list): List of bounding boxes.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Padded class id list and padded bounding box list.\n",
    "        \"\"\"\n",
    "        \n",
    "        padded_class_ids = keras.preprocessing.sequence.pad_sequences(self.cls_id_list, padding='post', dtype='int32')\n",
    "        padded_bbx = keras.preprocessing.sequence.pad_sequences(self.bbx_list, padding='post', dtype='float32')\n",
    "        \n",
    "        return padded_class_ids, padded_bbx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_images, train_class_ids, train_bboxes)\n",
    "train_ds = train_dl.load_train_dataset()\n",
    "train_ds = Preprocessor(train_ds).preprocess()\n",
    "train_ds = train_ds.repeat(3).batch(BATCH_SIZE)\\\n",
    "                .prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_ds.take(1):\n",
    "    image, cls = batch\n",
    "    print(cls.shape)\n",
    "    print(image.shape)\n",
    "    print(image[1].numpy().min(), image[1].numpy().max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_valid_dataset = AnnotationProcessor(annotation_file= str(VALIDATION_DIR/'_annotations.csv'))\n",
    "\n",
    "valid_image_paths, valid_class_ids, valid_bboxes  = prepare_valid_dataset.process_annotations(image_dir=VALIDATION_DIR, class_id_map=_class_map)\n",
    "len(valid_image_paths), len(valid_class_ids), len(valid_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dl = DataLoader(valid_image_paths, valid_class_ids, valid_bboxes).load_val_dataset()\n",
    "valid_ds = Preprocessor(valid_dl).preprocess()\n",
    "valid_ds = valid_ds.batch(BATCH_SIZE)\\\n",
    "                .prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in valid_ds.take(1):\n",
    "    image, cls, = batch\n",
    "    print(cls.shape)\n",
    "    print(image.shape)\n",
    "    print(image[1].numpy().min(), image[1].numpy().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "METRICS = [\n",
    "        tfa.image.iou,\n",
    "        tf.keras.metrics.MeanSquaredError(),\n",
    "        tf.keras.metrics.MeanAbsoluteError()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define  Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "to_monitor = 'val_loss'\n",
    "mode = 'min'\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, \n",
    "                                            patience=5, \n",
    "                                            monitor=to_monitor,\n",
    "                                            mode=mode,\n",
    "                                            min_lr=1e-6,\n",
    "                                            verbose=1),\n",
    "\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(str(CHECK_POINT_DIR), \"regressor_ckpt_{epoch}.keras\") ,\n",
    "                                        save_weights_only=False,\n",
    "                                        save_best_only=True,\n",
    "                                        monitor=to_monitor,\n",
    "                                        mode=mode,\n",
    "                                        verbose=1),\n",
    "                                        \n",
    "    tf.keras.callbacks.EarlyStopping(monitor=to_monitor, \n",
    "                                    patience=10,\n",
    "                                    mode=mode, \n",
    "                                    restore_best_weights=True),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ResNet50 Model Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.resnet50 import resnet50_regressor\n",
    "tf.keras.backend.clear_session()\n",
    "model = resnet50_regressor(input_shape=(IMG_SIZE,IMG_SIZE,3), num_classes=NUM_CLASSES)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building and Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss= tf.keras.losses.Huber(),\n",
    "    metrics=METRICS)  # Use IoU metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(\"/brain-tumor-resnet50_regressor\")\n",
    "mlflow.tensorflow.autolog(log_models=True, log_datasets=False)\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    # class_weight=class_weight_dict,\n",
    "    callbacks=[callbacks],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_training_results(history):\n",
    "    \"\"\"\n",
    "    Visualizes training and validation loss, and training and validation accuracy.\n",
    "\n",
    "    Args:\n",
    "        history: A dictionary or object containing training history data.\n",
    "                 For example, a Keras History object or a dictionary with keys:\n",
    "                 'loss', 'val_loss', 'accuracy', 'val_accuracy'.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(history, dict):\n",
    "        # Assumes history is a dictionary\n",
    "        loss = history.get('loss')\n",
    "        val_loss = history.get('val_loss')\n",
    "        accuracy = history.get('accuracy')\n",
    "        val_accuracy = history.get('val_accuracy')\n",
    "    else:\n",
    "        # Assumes history is a Keras History object or similar\n",
    "        loss = history.history.get('loss')\n",
    "        val_loss = history.history.get('val_loss')\n",
    "        accuracy = history.history.get('accuracy')\n",
    "        val_accuracy = history.history.get('val_accuracy')\n",
    "\n",
    "    if loss and val_loss:\n",
    "        epochs = range(1, len(loss) + 1)\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Plot training & validation loss values\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "        plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "        plt.title('Training and validation loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "    if accuracy and val_accuracy:\n",
    "        if not (loss and val_loss):\n",
    "          plt.figure(figsize=(12, 5))\n",
    "        else:\n",
    "          plt.subplot(1, 2, 2)\n",
    "        # Plot training & validation accuracy values\n",
    "        plt.plot(epochs, accuracy, 'r', label='Training accuracy')\n",
    "        plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "        plt.title('Training and validation accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout() #prevents overlapping titles/labels\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Datasets setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prepare_test_dataset = AnnotationProcessor(annotation_file= str(TEST_DIR/'_annotations.csv'))\n",
    "_class_map = {v: k for k, v in enumerate(CLASS_NAME)}\n",
    "test_image_paths, test_class_ids, test_bboxes = prepare_test_dataset.process_annotations(image_dir=TEST_DIR, class_id_map=_class_map)\n",
    "\n",
    "len(test_image_paths), len(test_class_ids), len(test_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(test_image_paths, test_class_ids, test_bboxes).load_val_dataset()\n",
    "test_ds = Preprocessor(test_dl).preprocess()\n",
    "test_ds = test_ds.batch(BATCH_SIZE)\\\n",
    "                .prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_ds, return_dict=True, steps=1)\n",
    "print(\"Testing accuracy: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "y_true = test_class_ids\n",
    "y_pred = model.predict(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_pred>0.5).astype(int)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_true_bin = mlb.fit_transform(y_true)\n",
    "y_pred_bin = mlb.transform(y_pred) #use transform, not fit_transform\n",
    "y_true_bin,y_pred_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true_bin, y_pred_bin, labels=[0,1,2], target_names=CLASS_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "auc_roc_values = []\n",
    "fig, axs = plt.subplots(1)\n",
    "for i in range(len(test_class_ids)):\n",
    "    try:\n",
    "        roc_score_per_label = metrics.roc_auc_score(y_true=y_true[:,i], y_score=y_pred_bin[:,i])\n",
    "        auc_roc_values.append(roc_score_per_label)\n",
    "        fpr, tpr, _ = metrics.roc_curve(y_true=y_true[:,i],  y_score=y_pred_bin[:,i])\n",
    "        \n",
    "        axs.plot([0,1], [0,1], 'k--')\n",
    "        axs.plot(fpr, tpr, \n",
    "                label=f'{CLASS_NAME[i]} - AUC = {round(roc_score_per_label, 3)}')\n",
    "\n",
    "        axs.set_xlabel('False Positive Rate')\n",
    "        axs.set_ylabel('True Positive Rate')\n",
    "        axs.legend(loc='lower right')\n",
    "    except:\n",
    "        print(\n",
    "            f\"Error in generating ROC curve for {CLASS_NAME[i]}. \"\n",
    "            f\"Dataset lacks enough examples.\"\n",
    "        )\n",
    "plt.savefig(f\"{cfg.OUTPUTS.OUPUT_DIR}/ROC-Curve.png\")\n",
    "mlflow.log_figure(fig, 'ROC-Curve.png')\n",
    "results = model.evaluate(test_ds, verbose=0,return_dict=True)\n",
    "mlflow.log_metrics(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
