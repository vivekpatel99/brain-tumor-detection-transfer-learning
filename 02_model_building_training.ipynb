{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Tumor Detection\n",
    "Description\n",
    "This dataset was originally created by Yousef Ghanem. To see the current project, which may have been updated since this version, please go here: https://universe.roboflow.com/yousef-ghanem-jzj4y/brain-tumor-detection-fpf1f.\n",
    "\n",
    "This dataset is part of RF100, an Intel-sponsored initiative to create a new object detection benchmark for model generalizability.\n",
    "\n",
    "Access the RF100 Github repo: https://github.com/roboflow-ai/roboflow-100-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')], '2.18.0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "tf.config.list_physical_devices('GPU'), tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualization_funcs import plot_random_images_bbox\n",
    "from utils.prepare_dataset import PrepareDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# auto reload dotenv \n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "# auto reload libs\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/brain-tumor-2/train/\n"
     ]
    }
   ],
   "source": [
    "from hydra import initialize, compose\n",
    "\n",
    "# https://gist.github.com/bdsaglam/586704a98336a0cf0a65a6e7c247d248\n",
    "\n",
    "with initialize(version_base=None, config_path=\"conf\"):\n",
    "    cfg = compose(config_name=\"config\")\n",
    "    print(cfg.DATASET_DIRS.TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TRAIN_DIR': '${DATASET.DATASET_DIR}/${DATASET.DATASET_NAME}/train/', 'VALIDATION_DIR': '${DATASET.DATASET_DIR}/${DATASET.DATASET_NAME}/valid', 'TEST_DIR': '${DATASET.DATASET_DIR}/${DATASET.DATASET_NAME}/test'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.DATASET_DIRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIRS = Path(cfg.DATASET.DATASET_DIR)\n",
    "TRAIN_DIR = Path(cfg.DATASET_DIRS.TRAIN_DIR)\n",
    "VALIDATION_DIR = Path(cfg.DATASET_DIRS.VALIDATION_DIR)\n",
    "TEST_DIR = Path(cfg.DATASET_DIRS.TEST_DIR)\n",
    "\n",
    "TRAIN_IMAGE_DIR = TRAIN_DIR / 'images'\n",
    "TRAIN_LABELS_DIR = TRAIN_DIR / 'labels'\n",
    "\n",
    "VALID_IMAGE_DIR = VALIDATION_DIR / 'images'\n",
    "VALID_LABELS_DIR = VALIDATION_DIR / 'labels'\n",
    "\n",
    "TEST_IMAGE = TEST_DIR / 'images'\n",
    "TEST_LABELS = TEST_DIR / 'labels'\n",
    "\n",
    "IMG_SIZE = cfg.TRAIN.IMG_SIZE\n",
    "BATCH_SIZE = cfg.TRAIN.BATCH_SIZE\n",
    "\n",
    "CLASS_NAME = [\n",
    "    'label0',\n",
    "    'label1',\n",
    "    'label2'\n",
    "]\n",
    "class_map = {k: v for k, v in enumerate(CLASS_NAME)}\n",
    "\n",
    "NUM_CLASSES = len(CLASS_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Download from Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_DIR.exists():\n",
    "    from roboflow import Roboflow\n",
    "    rf = Roboflow()\n",
    "    project = rf.workspace(\"roboflow-100\").project(\"brain-tumor-m2pbp\")\n",
    "    version = project.version(2)\n",
    "    dataset = version.download(\"yolov8\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8867b9077c254193bbaf581e6d007f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6930 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "prepare_train_dataset = PrepareDataset(image_dir=TRAIN_IMAGE_DIR, \n",
    "                                    label_dir=TRAIN_LABELS_DIR,\n",
    "                                    dst_img_size=(IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "train_images, train_class_ids, train_bboxes  = prepare_train_dataset.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_images), len(train_class_ids), len(train_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_images_bbox(image_paths=train_images, \n",
    "                        class_ids=train_class_ids, \n",
    "                        bboxes=train_bboxes,\n",
    "                        class_map=class_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebalance dataset by Down sempling to dataset with min images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebal_train_images, rebal_train_class_ids, rebal_train_bboxes = prepare_train_dataset.rebalance_by_down_sampling_datasets(augment=True)\n",
    "len(train_images), len(train_class_ids), len(train_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = tf.data.Dataset.from_tensor_slices(\n",
    "    (rebal_train_images, rebal_train_class_ids, rebal_train_bboxes))\n",
    "len(train_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    return image\n",
    "\n",
    "def load_dataset(image, class_id, bbox):\n",
    "    # images = load_image(image_path)\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "    # return   tf.cast(image, tf.float32), tf.cast(class_id, tf.int32), tf.cast(bbox, tf.float32) \n",
    "    return   tf.cast(image, tf.float32), (tf.one_hot(class_id,NUM_CLASSES), tf.cast(bbox, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_datasets.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_ds.take(1):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.shuffle(BATCH_SIZE * 4, reshuffle_each_iteration=True)\\\n",
    "                                .repeat() \\\n",
    "                                .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                                .prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation datasets setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_valid_datasets = PrepareDataset(image_dir=VALID_IMAGE_DIR,\n",
    "                                label_dir=VALID_LABELS_DIR)\n",
    "valid_image_paths, valid_class_ids, valid_bboxes = prepare_valid_datasets.get_dataset()\n",
    "len(valid_image_paths), len(valid_class_ids), len(valid_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datasets = tf.data.Dataset.from_tensor_slices((valid_image_paths,\n",
    "                                               valid_class_ids,\n",
    "                                               valid_bboxes))\n",
    "\n",
    "val_ds = val_datasets.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.repeat()\\\n",
    "                .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                .prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Datasets setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IMAGE = TEST_DIR/'images'\n",
    "TEST_LABELS = TEST_DIR/'labels'\n",
    "\n",
    "prepare_test_datasets = PrepareDataset(image_dir=TEST_IMAGE,\n",
    "                                label_dir=TEST_LABELS)\n",
    "test_image_paths, test_class_ids, test_bboxes = prepare_test_datasets.get_dataset()\n",
    "len(test_image_paths), len(test_class_ids), len(test_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datasets = tf.data.Dataset.from_tensor_slices((test_image_paths,\n",
    "                                               test_class_ids,\n",
    "                                               test_bboxes))\n",
    "\n",
    "test_ds = test_datasets.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.repeat()\\\n",
    "                .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                .prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define DenseNet121 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define DenseNet121 as a Feature Extractor\n",
    "def feature_extractor(inputs)-> tf.keras.Model:\n",
    "\n",
    "    inputs = tf.keras.applications.densenet.preprocess_input(inputs)\n",
    "    \n",
    "    # Create a DenseNet121 model object\n",
    "    densenet121 = tf.keras.applications.DenseNet121(\n",
    "        include_top = False, \n",
    "        weights = \"imagenet\",\n",
    "        input_shape = (IMG_SIZE, IMG_SIZE, 3),\n",
    "        input_tensor=inputs\n",
    "    )\n",
    "    \n",
    "    densenet121.trainable = False\n",
    "    for layer in densenet121.layers[:149]:\n",
    "        layer.trainable = False\n",
    "    for layer in densenet121.layers[149:]:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    feature_extractor = densenet121.output\n",
    "    return feature_extractor\n",
    "\n",
    "\n",
    "### Define Dense Layers\n",
    "def dense_layers(features)->tf.keras.Layer:\n",
    "    l2 = tf.keras.regularizers.l2\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(256, (3,3), activation='relu')(features)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPool2D((2,2))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(128, (3,3), activation='relu')(features)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPool2D((2,2))(x)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(units=1024, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "    x = tf.keras.layers.Dense(units=512, activation='relu',  kernel_regularizer=l2(0.01))(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "### Define Bounding Box Regression\n",
    "def bounding_box_regression(x)->tf.keras.Layer:\n",
    "\n",
    "    # Dense layer named `bounding_box`\n",
    "    bounding_box_regression_output = tf.keras.layers.Dense(units=4, name='bounding_box')(x)\n",
    "\n",
    "    return bounding_box_regression_output\n",
    "\n",
    "###Define Classifier Layer\n",
    "def classifer(inputs)->tf.keras.Model:\n",
    "    return tf.keras.layers.Dense(units=NUM_CLASSES, activation='softmax', name = 'classification')(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobv2_feature_extractor(inputs):\n",
    "    \n",
    "    inputs = tf.keras.applications.mobilenet.preprocess_input(inputs)\n",
    "\n",
    "    # Create a mobilenet version 2 model object\n",
    "    mobilenet_model = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=(224, 224, 3),\n",
    "        weights='imagenet',\n",
    "        include_top=False\n",
    "    )\n",
    "\n",
    "    # pass the inputs into this modle object to get a feature extractor for these inputs\n",
    "    feature_extractor = mobilenet_model(inputs)\n",
    "\n",
    "    # return the feature_extractor\n",
    "    return feature_extractor#.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_model()->tf.keras.Model:\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "    _feature_extractor = feature_extractor(inputs)\n",
    "    # _feature_extractor = mobv2_feature_extractor(inputs)\n",
    "    \n",
    "    dense_output = dense_layers(_feature_extractor)\n",
    "\n",
    "    bounding_box_regression_output = bounding_box_regression(dense_output)\n",
    "\n",
    "    classification_output = classifer(dense_output)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, \n",
    "                          outputs=[classification_output, \n",
    "                                   bounding_box_regression_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define  Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                  mode='min',\n",
    "                                                  patience=2,\n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_classification_accuracy', \n",
    "                                                 factor=0.2, \n",
    "                                                 patience=2, \n",
    "                                                 verbose=1, \n",
    "                                                 mode='auto', \n",
    "                                                 min_delta=0.0001, \n",
    "                                                 cooldown=0, \n",
    "                                                 min_lr=0.001)\n",
    "model_checkpoint =  tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./training_checkpoints/ckpt_{epoch}.weights.h5',\n",
    "        save_weights_only=True),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-2\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, use_ema=True)\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building and Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet121_model = final_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet121_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss={'classification': 'categorical_crossentropy', 'bounding_box': 'mse'},\n",
    "    metrics={'classification': 'accuracy', 'bounding_box': 'mse'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# densenet121_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Get the length of the training set\n",
    "length_of_training_dataset = len(train_datasets)\n",
    "\n",
    "# Get the length of the validation set\n",
    "length_of_validation_dataset = len(val_datasets)\n",
    "\n",
    "# Get the steps per epoch \n",
    "steps_per_epoch = math.ceil(length_of_training_dataset/BATCH_SIZE)\n",
    "\n",
    "# get the validation steps (per epoch) \n",
    "validation_steps = math.ceil(length_of_validation_dataset/BATCH_SIZE)\n",
    "validation_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = densenet121_model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[reduce_lr, early_stopping, model_checkpoint],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, classification_loss, bounding_box_loss, classification_accuracy, bounding_box_mse = densenet121_model.evaluate(test_ds, steps=1)\n",
    "print(\"Testing accuracy: \", classification_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metric_name, title, ylim=1):\n",
    "  plt.title(title)\n",
    "  plt.ylim(0,ylim)\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.legend([metric_name, 'val_' + metric_name])\n",
    "  plt.plot(history.history[metric_name],color='blue',label=metric_name)\n",
    "  plt.plot(history.history['val_' + metric_name],color='green',label='val_' + metric_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(\"bounding_box_loss\", \"bounding_box_loss\", ylim=0.2)\n",
    "plot_metrics(\"classification_accuracy\", \"accuracy\", ylim=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(\"classification_loss\", \"Classification Loss\")\n",
    "plot_metrics(\"bounding_box_loss\", \"Bounding Box Loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
