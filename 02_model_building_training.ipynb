{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Tumor Detection\n",
    "Description\n",
    "This dataset was originally created by Yousef Ghanem. To see the current project, which may have been updated since this version, please go here: https://universe.roboflow.com/yousef-ghanem-jzj4y/brain-tumor-detection-fpf1f.\n",
    "\n",
    "This dataset is part of RF100, an Intel-sponsored initiative to create a new object detection benchmark for model generalizability.\n",
    "\n",
    "Access the RF100 Github repo: https://github.com/roboflow-ai/roboflow-100-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 13:01:57.591404: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741784517.604663   69372 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741784517.608476   69372 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1741784517.619306   69372 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1741784517.619318   69372 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1741784517.619319   69372 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1741784517.619320   69372 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-12 13:01:57.622857: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')], '2.19.0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_gpu = tf.config.list_physical_devices('GPU')\n",
    "if not found_gpu:\n",
    "    raise Exception(\"No GPU found\")\n",
    "found_gpu, tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualization_funcs import plot_random_images_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto reload dotenv \n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "# auto reload libs\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/brain-tumor-2/train/\n"
     ]
    }
   ],
   "source": [
    "from hydra import initialize, compose\n",
    "\n",
    "# https://gist.github.com/bdsaglam/586704a98336a0cf0a65a6e7c247d248\n",
    "\n",
    "with initialize(version_base=None, config_path=\"conf\"):\n",
    "    cfg = compose(config_name=\"config\")\n",
    "    print(cfg.DATASET_DIRS.TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TRAIN_DIR': '${DATASET.DATASET_DIR}/${DATASET.DATASET_NAME}/train/', 'VALIDATION_DIR': '${DATASET.DATASET_DIR}/${DATASET.DATASET_NAME}/valid', 'TEST_DIR': '${DATASET.DATASET_DIR}/${DATASET.DATASET_NAME}/test'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.DATASET_DIRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIRS = Path(cfg.DATASET.DATASET_DIR)\n",
    "TRAIN_DIR = Path(cfg.DATASET_DIRS.TRAIN_DIR)\n",
    "VALIDATION_DIR = Path(cfg.DATASET_DIRS.VALIDATION_DIR)\n",
    "TEST_DIR = Path(cfg.DATASET_DIRS.TEST_DIR)\n",
    "\n",
    "\n",
    "IMG_SIZE = cfg.TRAIN.IMG_SIZE\n",
    "BATCH_SIZE = cfg.TRAIN.BATCH_SIZE\n",
    "LOG_DIR = cfg.OUTPUTS.LOG_DIR\n",
    "CHECK_POINT_DIR = Path(cfg.OUTPUTS.CHECKPOINT_PATH)\n",
    "CLASS_NAME = [\n",
    "    'label0',\n",
    "    'label1',\n",
    "    'label2'\n",
    "]\n",
    "class_map = {k: v for k, v in enumerate(CLASS_NAME)}\n",
    "\n",
    "NUM_EPOCHS = cfg.TRAIN.NUM_EPOCHS\n",
    "LEARNING_RATE = cfg.TRAIN.LEARNING_RATE\n",
    "\n",
    "NUM_CLASSES = len(CLASS_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Download from Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_DIR.exists():\n",
    "    from roboflow import Roboflow\n",
    "    rf = Roboflow()\n",
    "    project = rf.workspace(\"roboflow-100\").project(\"brain-tumor-m2pbp\")\n",
    "    version = project.version(2)\n",
    "    dataset = version.download(\"tensorflow\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.prepare_dataset import AnnotationProcessor\n",
    "\n",
    "prepare_train_dataset = AnnotationProcessor(annotation_file= str(TRAIN_DIR/'_annotations.csv'))\n",
    "_class_map = {v: k for k, v in enumerate(CLASS_NAME)}\n",
    "train_images, train_class_ids, train_bboxes  = prepare_train_dataset.process_annotations(image_dir=TRAIN_DIR, class_id_map=_class_map)\n",
    "\n",
    "len(train_images), len(train_class_ids), len(train_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57916667, 0.33333333, 0.75833333, 0.425     ],\n",
       "       [0.5375    , 0.275     , 0.82916667, 0.5125    ],\n",
       "       [0.57083333, 0.32083333, 0.76666667, 0.45416667]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bboxes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebalance dataset by Down sempling to dataset with min images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebal_train_images, rebal_train_class_ids, rebal_train_bboxes = prepare_train_dataset.rebalance_by_down_sampling_datasets(augment=False)\n",
    "# len(train_images), len(train_class_ids), len(train_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_class_ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_cls_id_bbx(class_id_list, bbox_list):\n",
    "    padded_class_ids = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    class_id_list, padding='post', dtype='int32')\n",
    "    padded_bbx = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        bbox_list, padding='post', dtype='float32')\n",
    "    \n",
    "    return padded_class_ids, padded_bbx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.5208333 , 0.51666665, 0.6791667 , 0.6125    ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ]], dtype=float32),\n",
       " 100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_class_ids, padded_bbx = pad_cls_id_bbx(train_class_ids, train_bboxes)\n",
    "padded_bbx[1], len(padded_bbx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 0], dtype=int32), 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_class_ids[1], len(padded_class_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1741784521.075345   69372 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6938 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:0a:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_images, padded_class_ids, padded_bbx))\n",
    "del train_images, train_class_ids, train_bboxes, padded_class_ids, padded_bbx\n",
    "len(train_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    return image\n",
    "\n",
    "def load_dataset(image, class_ids, bbox):\n",
    "    tf_image = load_image(image)\n",
    "    multi_hot = tf.reduce_sum(\n",
    "    tf.one_hot(class_ids, NUM_CLASSES), \n",
    "    axis=0\n",
    ")  # Shape: (NUM_CLASSES,)\n",
    "    return  tf_image, multi_hot, bbox\n",
    "\n",
    "def preprocess(image_batch, class_ids, bbox):\n",
    "    processed_image_batch = tf.keras.applications.resnet.preprocess_input(image_batch)\n",
    "    return processed_image_batch, (class_ids, bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomBrightness(0.1),\n",
    "    layers.RandomContrast(0.1),\n",
    "    layers.RandomSaturation(0.1),\n",
    "    layers.RandomHue(0.1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_datasets.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE) \n",
    "train_ds = train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.map(lambda x,y: (data_augmentation(x), y), num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 240, 3) tf.Tensor([1. 1. 1.], shape=(3,), dtype=float32) (3, 4)\n",
      "<dtype: 'float32'> <dtype: 'float32'> <dtype: 'float32'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 13:02:01.646973: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for batch in train_ds.take(1):\n",
    "    image, (class_id, bbx) = batch\n",
    "    print(image.shape, class_id, bbx.shape)\n",
    "    print(image.dtype, class_id.dtype, bbx.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.shuffle(buffer_size=train_datasets.cardinality().numpy(), reshuffle_each_iteration=True)\\\n",
    "                                .batch(BATCH_SIZE)\\\n",
    "                                .prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 255.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 13:02:02.531290: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for batch in train_ds.take(1):\n",
    "    image, (class_id, bbx) = batch\n",
    "    print(image.numpy().min(), image.numpy().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation datasets setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 100)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_train_dataset = AnnotationProcessor(annotation_file= str(VALIDATION_DIR/'_annotations.csv'))\n",
    "\n",
    "valid_image_paths, valid_class_ids, valid_bboxes  = prepare_train_dataset.process_annotations(image_dir=VALIDATION_DIR, class_id_map=_class_map)\n",
    "len(valid_image_paths), len(valid_class_ids), len(valid_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_padded_class_ids, valid_padded_bbx = pad_cls_id_bbx(valid_class_ids, valid_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datasets = tf.data.Dataset.from_tensor_slices((valid_image_paths,\n",
    "                                               valid_padded_class_ids,\n",
    "                                               valid_padded_bbx))\n",
    "\n",
    "valid_ds = val_datasets.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "valid_ds = valid_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                .batch(BATCH_SIZE)\\\n",
    "                .prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "for batch in valid_ds.take(1):\n",
    "    image, (cls, bbx) = batch\n",
    "    print(bbx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ResNet50 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "### Define ResNet50 as a Feature Extractor\n",
    "def feature_extractor(inputs)-> tf.keras.Model:\n",
    "    resnet50 = tf.keras.applications.ResNet50(\n",
    "        include_top = False, \n",
    "        weights = \"imagenet\",\n",
    "        input_shape = input_shape,    \n",
    "        input_tensor=inputs\n",
    "    )\n",
    "    resnet50.trainable = True\n",
    "    for layer in resnet50.layers[:140]: #example number of layers to freeze\n",
    "        layer.trainable = False\n",
    "    feature_extractor = resnet50.output\n",
    "    return feature_extractor\n",
    "\n",
    "\n",
    "### Define Dense Layers\n",
    "def dense_layers(features)->tf.keras.Layer:\n",
    "    x = keras.layers.Conv2D(filters=256, kernel_size=(1, 1), activation='relu')(features) # 1x1 conv\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(units=1024, activation='relu', kernel_regularizer='l2')(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    x = keras.layers.Dense(units=512, activation='relu', kernel_regularizer='l2')(x)\n",
    "    return x\n",
    "\n",
    "### Define Bounding Box Regression\n",
    "def bounding_box_regression(x)->tf.keras.Layer:\n",
    "    bbox_shape=4\n",
    "    bounding_box_regression_output = tf.keras.layers.Dense(units=bbox_shape*NUM_CLASSES, name='_bounding_box', activation='linear')(x)\n",
    "    reshape_bbox = tf.keras.layers.Reshape(\n",
    "        (NUM_CLASSES, 4),  # Not hard-coded\n",
    "        name='bounding_box'\n",
    "    )(bounding_box_regression_output)\n",
    "    return reshape_bbox\n",
    "\n",
    "###Define Classifier Layer\n",
    "def classifer(inputs)->tf.keras.Model:\n",
    "    return tf.keras.layers.Dense(units=NUM_CLASSES, activation='sigmoid', name = 'classification')(inputs)\n",
    "\n",
    "def final_model()->tf.keras.Model:\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    _feature_extractor = feature_extractor(inputs)\n",
    "   \n",
    "    dense_output = dense_layers(_feature_extractor)\n",
    "\n",
    "    bounding_box_regression_output = bounding_box_regression(dense_output)\n",
    "\n",
    "    classification_output = classifer(dense_output)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, \n",
    "                          outputs=[classification_output, \n",
    "                                   bounding_box_regression_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobv2_feature_extractor(inputs):\n",
    "    \n",
    "    inputs = tf.keras.applications.mobilenet.preprocess_input(inputs)\n",
    "\n",
    "    # Create a mobilenet version 2 model object\n",
    "    mobilenet_model = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=(224, 224, 3),\n",
    "        weights='imagenet',\n",
    "        include_top=False\n",
    "    )\n",
    "\n",
    "    # pass the inputs into this modle object to get a feature extractor for these inputs\n",
    "    feature_extractor = mobilenet_model(inputs)\n",
    "\n",
    "    # return the feature_extractor\n",
    "    return feature_extractor#.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall'),\n",
    "    tf.keras.metrics.AUC(name='AUC', multi_label=True), \n",
    "    tf.keras.metrics.F1Score(name='f1_score',average='weighted'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define  Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "to_monitor = 'val_loss'\n",
    "mode = 'min'\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, \n",
    "                                            patience=2, \n",
    "                                            monitor=to_monitor,\n",
    "                                            mode=mode,\n",
    "                                            min_lr=1e-6,\n",
    "                                            verbose=1),\n",
    "\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(str(CHECK_POINT_DIR), \"ckpt_{epoch}.keras\") ,\n",
    "                                        save_weights_only=False,\n",
    "                                        save_best_only=True,\n",
    "                                        monitor=to_monitor,\n",
    "                                        mode=mode,\n",
    "                                        verbose=1),\n",
    "                                        \n",
    "    tf.keras.callbacks.EarlyStopping(monitor=to_monitor, \n",
    "                                    patience=10,\n",
    "                                    mode=mode, \n",
    "                                    restore_best_weights=True),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, use_ema=True)\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE, momentum=0.9, clipvalue=1.0)\n",
    "\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9, clipvalue=1.0)\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=cfg.TRAIN.LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building and Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def iou_loss(y_true, y_pred):  # Assuming y_true and y_pred are (batch_size, 4)\n",
    "    # y_true = y_true[0]\n",
    "    # y_pred = tf.reshape(y_pred, ( 3, 4))\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32) # Cast to float32\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32) # Cast to float32\n",
    "\n",
    "    x_true = y_true[..., 0]\n",
    "    y_true_ = y_true[..., 1]\n",
    "    x_max_true = y_true[..., 2]\n",
    "    y_max_true = y_true[..., 3]\n",
    "\n",
    "    x_pred = y_pred[..., 0]\n",
    "    y_pred_ = y_pred[..., 1]\n",
    "    x_max_pred = y_pred[..., 2]\n",
    "    y_max_pred = y_pred[..., 3]\n",
    "\n",
    "    area_true = (x_max_true - x_true) * (y_max_true - y_true_)\n",
    "    area_pred = (x_max_pred - x_pred) * (y_max_pred - y_pred_)\n",
    "\n",
    "    x_intersect = tf.maximum(x_true, x_pred)\n",
    "    y_intersect = tf.maximum(y_true_, y_pred_)\n",
    "    x_max_intersect = tf.minimum(x_max_true, x_max_pred)\n",
    "    y_max_intersect = tf.minimum(y_max_true, y_max_pred)\n",
    "\n",
    "    area_intersect = tf.maximum(0.0, x_max_intersect - x_intersect) * tf.maximum(0.0, y_max_intersect - y_intersect) # avoid negative values\n",
    "    iou = area_intersect / (area_true + area_pred - area_intersect + 1e-7)  # Add small epsilon for numerical stability\n",
    "    return 1.0 - iou  # We want to *minimize* the loss\n",
    "\n",
    "def iou_metric(y_true, y_pred):  # No negation for metric\n",
    "    # y_true = y_true[0]\n",
    "    # y_pred = tf.reshape(y_pred, (3, 4))\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32) # Cast to float32\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32) # Cast to float32\n",
    "\n",
    "    x_true = y_true[..., 0]\n",
    "    y_true_ = y_true[..., 1]\n",
    "    x_max_true = y_true[..., 2]\n",
    "    y_max_true = y_true[..., 3]\n",
    "\n",
    "    x_pred = y_pred[..., 0]\n",
    "    y_pred_ = y_pred[..., 1]\n",
    "    x_max_pred = y_pred[..., 2]\n",
    "    y_max_pred = y_pred[..., 3]\n",
    "\n",
    "    area_true = (x_max_true - x_true) * (y_max_true - y_true_)\n",
    "    area_pred = (x_max_pred - x_pred) * (y_max_pred - y_pred_)\n",
    "\n",
    "    x_intersect = tf.maximum(x_true, x_pred)\n",
    "    y_intersect = tf.maximum(y_true_, y_pred_)\n",
    "    x_max_intersect = tf.minimum(x_max_true, x_max_pred)\n",
    "    y_max_intersect = tf.minimum(y_max_true, y_max_pred)\n",
    "\n",
    "    area_intersect = tf.maximum(0.0, x_max_intersect - x_intersect) * tf.maximum(0.0, y_max_intersect - y_intersect) # avoid negative values\n",
    "    iou = area_intersect / (area_true + area_pred - area_intersect + 1e-7)  # Add small epsilon for numerical stability\n",
    "    return iou  # Return IoU directly for metric\n",
    "\n",
    "def bounding_box_loss(y_true, y_pred):\n",
    "    # Reshape the output to (batch_size, 3, 4)\n",
    "    y_pred = tf.reshape(y_pred, (-1, 3, 4))\n",
    "    #y_true should be of shape (batch_size, 3, 4)\n",
    "    loss = 0\n",
    "    for tr_bb, pr_bb in zip(y_true, y_pred):\n",
    "        loss += tf.keras.losses.MeanSquaredError()(tr_bb[0, :], pr_bb[0,:])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = final_model()\n",
    "# model.compile(\n",
    "#     optimizer=optimizer,\n",
    "#     loss={'classification': 'binary_crossentropy', 'bounding_box': iou_loss},\n",
    "#     metrics={'classification': METRICS, 'bounding_box': iou_metric})  # Use IoU metric\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss={'classification': 'binary_crossentropy', 'bounding_box': 'mse'},\n",
    "    metrics={'classification': METRICS, 'bounding_box': 'mse'})  # Use IoU metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/12 13:02:05 WARNING mlflow.utils.autologging_utils: MLflow tensorflow autologging is known to be compatible with 2.7.4 <= tensorflow <= 2.18.0, but the installed version is 2.19.0. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a compatible version, or try upgrading MLflow.\n",
      "2025/03/12 13:02:05 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '05650f7daa4c44c39081b84b01d4bdb9', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current tensorflow workflow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1741784533.062166   69494 service.cc:152] XLA service 0x71760c003ac0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1741784533.062184   69494 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3080, Compute Capability 8.6\n",
      "2025-03-12 13:02:13.331600: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1741784534.673749   69494 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-03-12 13:02:15.807652: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2025-03-12 13:02:16.066180: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot', 264 bytes spill stores, 264 bytes spill loads\n",
      "\n",
      "2025-03-12 13:02:16.265464: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8554', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2025-03-12 13:02:16.826180: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8554', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/7\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - bounding_box_loss: 0.2429 - bounding_box_mse: 0.2429 - classification_AUC: 0.1141 - classification_f1_score: 0.4990 - classification_loss: 0.6675 - classification_precision: 0.8400 - classification_recall: 0.5433 - loss: 11.8400 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1741784541.901596   69494 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - bounding_box_loss: 0.2412 - bounding_box_mse: 0.2412 - classification_AUC: 0.1335 - classification_f1_score: 0.5057 - classification_loss: 0.6588 - classification_precision: 0.8440 - classification_recall: 0.5710 - loss: 11.8122"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 13:02:24.286630: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7395', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-03-12 13:02:24.743360: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8554', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2025-03-12 13:02:25.022270: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot', 256 bytes spill stores, 256 bytes spill loads\n",
      "\n",
      "2025-03-12 13:02:25.147685: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot', 124 bytes spill stores, 124 bytes spill loads\n",
      "\n",
      "2025-03-12 13:02:25.414251: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8554', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - bounding_box_loss: 0.2374 - bounding_box_mse: 0.2368 - classification_AUC: 0.1566 - classification_f1_score: 0.5190 - classification_loss: 0.6477 - classification_precision: 0.8548 - classification_recall: 0.6019 - loss: 11.7828  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 13:02:32.122961: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1819_0', 208 bytes spill stores, 400 bytes spill loads\n",
      "\n",
      "2025-03-12 13:02:32.164290: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1819', 40 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2025-03-12 13:02:32.203832: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1819', 80 bytes spill stores, 144 bytes spill loads\n",
      "\n",
      "2025-03-12 13:02:32.262230: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1819', 608 bytes spill stores, 608 bytes spill loads\n",
      "\n",
      "2025-03-12 13:02:35.031161: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1819', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-03-12 13:02:35.049139: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1819', 212 bytes spill stores, 244 bytes spill loads\n",
      "\n",
      "2025-03-12 13:02:35.050195: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1819_0', 208 bytes spill stores, 392 bytes spill loads\n",
      "\n",
      "2025-03-12 13:02:35.129208: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1819', 428 bytes spill stores, 532 bytes spill loads\n",
      "\n",
      "2025-03-12 13:02:35.160999: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1819', 388 bytes spill stores, 388 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 11.34957, saving model to output/checkpoints/ckpt_1.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 3s/step - bounding_box_loss: 0.2366 - bounding_box_mse: 0.2356 - classification_AUC: 0.1647 - classification_f1_score: 0.5232 - classification_loss: 0.6434 - classification_precision: 0.8586 - classification_recall: 0.6124 - loss: 11.7731 - val_bounding_box_loss: 0.1976 - val_bounding_box_mse: 0.1970 - val_classification_AUC: 0.1680 - val_classification_f1_score: 0.5280 - val_classification_loss: 0.4395 - val_classification_precision: 0.8741 - val_classification_recall: 0.9772 - val_loss: 11.3496 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - bounding_box_loss: 0.1745 - bounding_box_mse: 0.1736 - classification_AUC: 0.3699 - classification_f1_score: 0.6267 - classification_loss: 0.4900 - classification_precision: 0.9087 - classification_recall: 0.8934 - loss: 11.3250\n",
      "Epoch 2: val_loss improved from 11.34957 to 10.87888, saving model to output/checkpoints/ckpt_2.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 347ms/step - bounding_box_loss: 0.1750 - bounding_box_mse: 0.1735 - classification_AUC: 0.3863 - classification_f1_score: 0.6241 - classification_loss: 0.4878 - classification_precision: 0.9094 - classification_recall: 0.8961 - loss: 11.3176 - val_bounding_box_loss: 0.1547 - val_bounding_box_mse: 0.1549 - val_classification_AUC: 0.1963 - val_classification_f1_score: 0.5280 - val_classification_loss: 0.2279 - val_classification_precision: 0.8767 - val_classification_recall: 1.0000 - val_loss: 10.8789 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - bounding_box_loss: 0.1403 - bounding_box_mse: 0.1411 - classification_AUC: 0.3183 - classification_f1_score: 0.5440 - classification_loss: 0.3201 - classification_precision: 0.9267 - classification_recall: 0.9638 - loss: 10.9004\n",
      "Epoch 3: val_loss improved from 10.87888 to 10.50918, saving model to output/checkpoints/ckpt_3.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 341ms/step - bounding_box_loss: 0.1384 - bounding_box_mse: 0.1397 - classification_AUC: 0.3370 - classification_f1_score: 0.5461 - classification_loss: 0.3150 - classification_precision: 0.9253 - classification_recall: 0.9635 - loss: 10.8902 - val_bounding_box_loss: 0.1338 - val_bounding_box_mse: 0.1336 - val_classification_AUC: 0.2052 - val_classification_f1_score: 0.5280 - val_classification_loss: 0.0915 - val_classification_precision: 0.8796 - val_classification_recall: 1.0000 - val_loss: 10.5092 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - bounding_box_loss: 0.1291 - bounding_box_mse: 0.1291 - classification_AUC: 0.3017 - classification_f1_score: 0.5848 - classification_loss: 0.0836 - classification_precision: 0.8950 - classification_recall: 0.9680 - loss: 10.4368\n",
      "Epoch 4: val_loss improved from 10.50918 to 10.16634, saving model to output/checkpoints/ckpt_4.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 335ms/step - bounding_box_loss: 0.1265 - bounding_box_mse: 0.1266 - classification_AUC: 0.3275 - classification_f1_score: 0.5811 - classification_loss: 0.0835 - classification_precision: 0.8967 - classification_recall: 0.9672 - loss: 10.4297 - val_bounding_box_loss: 0.1170 - val_bounding_box_mse: 0.1165 - val_classification_AUC: 0.2184 - val_classification_f1_score: 0.5280 - val_classification_loss: -0.0315 - val_classification_precision: 0.8855 - val_classification_recall: 1.0000 - val_loss: 10.1663 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - bounding_box_loss: 0.1047 - bounding_box_mse: 0.1047 - classification_AUC: 0.4431 - classification_f1_score: 0.5666 - classification_loss: -0.1140 - classification_precision: 0.9056 - classification_recall: 0.9556 - loss: 10.0113\n",
      "Epoch 5: val_loss improved from 10.16634 to 9.87201, saving model to output/checkpoints/ckpt_5.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 397ms/step - bounding_box_loss: 0.1054 - bounding_box_mse: 0.1054 - classification_AUC: 0.4486 - classification_f1_score: 0.5652 - classification_loss: -0.1173 - classification_precision: 0.9057 - classification_recall: 0.9573 - loss: 10.0042 - val_bounding_box_loss: 0.1090 - val_bounding_box_mse: 0.1075 - val_classification_AUC: 0.2224 - val_classification_f1_score: 0.5280 - val_classification_loss: -0.1253 - val_classification_precision: 0.8885 - val_classification_recall: 1.0000 - val_loss: 9.8720 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - bounding_box_loss: 0.1215 - bounding_box_mse: 0.1218 - classification_AUC: 0.4263 - classification_f1_score: 0.5722 - classification_loss: -0.3685 - classification_precision: 0.8774 - classification_recall: 0.9768 - loss: 9.5806\n",
      "Epoch 6: val_loss improved from 9.87201 to 9.59485, saving model to output/checkpoints/ckpt_6.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 335ms/step - bounding_box_loss: 0.1209 - bounding_box_mse: 0.1216 - classification_AUC: 0.4416 - classification_f1_score: 0.5701 - classification_loss: -0.3723 - classification_precision: 0.8798 - classification_recall: 0.9768 - loss: 9.5727 - val_bounding_box_loss: 0.1178 - val_bounding_box_mse: 0.1159 - val_classification_AUC: 0.2217 - val_classification_f1_score: 0.5280 - val_classification_loss: -0.2221 - val_classification_precision: 0.9149 - val_classification_recall: 0.9810 - val_loss: 9.5948 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - bounding_box_loss: 0.1239 - bounding_box_mse: 0.1243 - classification_AUC: 0.3187 - classification_f1_score: 0.5678 - classification_loss: -0.6691 - classification_precision: 0.9103 - classification_recall: 0.9938 - loss: 9.0855\n",
      "Epoch 7: val_loss improved from 9.59485 to 9.11870, saving model to output/checkpoints/ckpt_7.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 336ms/step - bounding_box_loss: 0.1237 - bounding_box_mse: 0.1245 - classification_AUC: 0.3369 - classification_f1_score: 0.5662 - classification_loss: -0.6573 - classification_precision: 0.9107 - classification_recall: 0.9926 - loss: 9.0834 - val_bounding_box_loss: 0.1343 - val_bounding_box_mse: 0.1331 - val_classification_AUC: 0.2257 - val_classification_f1_score: 0.5280 - val_classification_loss: -0.5240 - val_classification_precision: 0.9206 - val_classification_recall: 0.9696 - val_loss: 9.1187 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - bounding_box_loss: 0.1587 - bounding_box_mse: 0.1592 - classification_AUC: 0.5407 - classification_f1_score: 0.5702 - classification_loss: -1.0251 - classification_precision: 0.8853 - classification_recall: 0.9731 - loss: 8.6039\n",
      "Epoch 8: val_loss improved from 9.11870 to 8.91620, saving model to output/checkpoints/ckpt_8.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 354ms/step - bounding_box_loss: 0.1579 - bounding_box_mse: 0.1588 - classification_AUC: 0.5437 - classification_f1_score: 0.5683 - classification_loss: -1.0316 - classification_precision: 0.8863 - classification_recall: 0.9735 - loss: 8.5915 - val_bounding_box_loss: 0.1312 - val_bounding_box_mse: 0.1297 - val_classification_AUC: 0.2298 - val_classification_f1_score: 0.5280 - val_classification_loss: -0.5676 - val_classification_precision: 0.9206 - val_classification_recall: 0.9696 - val_loss: 8.9162 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - bounding_box_loss: 0.1710 - bounding_box_mse: 0.1708 - classification_AUC: 0.3770 - classification_f1_score: 0.5594 - classification_loss: -1.4704 - classification_precision: 0.9075 - classification_recall: 0.9757 - loss: 8.0285\n",
      "Epoch 9: val_loss improved from 8.91620 to 8.58426, saving model to output/checkpoints/ckpt_9.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 332ms/step - bounding_box_loss: 0.1722 - bounding_box_mse: 0.1719 - classification_AUC: 0.3800 - classification_f1_score: 0.5589 - classification_loss: -1.4948 - classification_precision: 0.9082 - classification_recall: 0.9758 - loss: 8.0127 - val_bounding_box_loss: 0.1327 - val_bounding_box_mse: 0.1305 - val_classification_AUC: 0.2302 - val_classification_f1_score: 0.5280 - val_classification_loss: -0.7671 - val_classification_precision: 0.9170 - val_classification_recall: 0.9658 - val_loss: 8.5843 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - bounding_box_loss: 0.1875 - bounding_box_mse: 0.1878 - classification_AUC: 0.3395 - classification_f1_score: 0.5622 - classification_loss: -2.2020 - classification_precision: 0.9350 - classification_recall: 0.9848 - loss: 7.1381\n",
      "Epoch 10: val_loss improved from 8.58426 to 7.78649, saving model to output/checkpoints/ckpt_10.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 361ms/step - bounding_box_loss: 0.1878 - bounding_box_mse: 0.1884 - classification_AUC: 0.3469 - classification_f1_score: 0.5613 - classification_loss: -2.1907 - classification_precision: 0.9352 - classification_recall: 0.9842 - loss: 7.1336 - val_bounding_box_loss: 0.1483 - val_bounding_box_mse: 0.1463 - val_classification_AUC: 0.2365 - val_classification_f1_score: 0.5280 - val_classification_loss: -1.4088 - val_classification_precision: 0.9328 - val_classification_recall: 0.9506 - val_loss: 7.7865 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - bounding_box_loss: 0.2109 - bounding_box_mse: 0.2108 - classification_AUC: 0.3698 - classification_f1_score: 0.5516 - classification_loss: -2.6162 - classification_precision: 0.9585 - classification_recall: 0.9782 - loss: 6.6651\n",
      "Epoch 11: val_loss improved from 7.78649 to 7.52385, saving model to output/checkpoints/ckpt_11.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 331ms/step - bounding_box_loss: 0.2128 - bounding_box_mse: 0.2126 - classification_AUC: 0.3926 - classification_f1_score: 0.5520 - classification_loss: -2.6614 - classification_precision: 0.9567 - classification_recall: 0.9775 - loss: 6.6392 - val_bounding_box_loss: 0.1556 - val_bounding_box_mse: 0.1542 - val_classification_AUC: 0.2424 - val_classification_f1_score: 0.5280 - val_classification_loss: -1.6662 - val_classification_precision: 0.9639 - val_classification_recall: 0.9125 - val_loss: 7.5238 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - bounding_box_loss: 0.2498 - bounding_box_mse: 0.2500 - classification_AUC: 0.3695 - classification_f1_score: 0.5566 - classification_loss: -4.0502 - classification_precision: 0.9414 - classification_recall: 0.9944 - loss: 5.0597\n",
      "Epoch 12: val_loss improved from 7.52385 to 7.07566, saving model to output/checkpoints/ckpt_12.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 336ms/step - bounding_box_loss: 0.2518 - bounding_box_mse: 0.2522 - classification_AUC: 0.3696 - classification_f1_score: 0.5565 - classification_loss: -4.0121 - classification_precision: 0.9413 - classification_recall: 0.9931 - loss: 5.0446 - val_bounding_box_loss: 0.1266 - val_bounding_box_mse: 0.1250 - val_classification_AUC: 0.2350 - val_classification_f1_score: 0.5280 - val_classification_loss: -1.8930 - val_classification_precision: 0.9228 - val_classification_recall: 0.9544 - val_loss: 7.0757 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - bounding_box_loss: 0.3868 - bounding_box_mse: 0.3903 - classification_AUC: 0.3849 - classification_f1_score: 0.5548 - classification_loss: -5.8884 - classification_precision: 0.9716 - classification_recall: 0.9935 - loss: 3.2737\n",
      "Epoch 13: val_loss improved from 7.07566 to 6.00939, saving model to output/checkpoints/ckpt_13.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 350ms/step - bounding_box_loss: 0.3806 - bounding_box_mse: 0.3867 - classification_AUC: 0.3879 - classification_f1_score: 0.5549 - classification_loss: -5.8718 - classification_precision: 0.9718 - classification_recall: 0.9933 - loss: 3.2389 - val_bounding_box_loss: 0.1498 - val_bounding_box_mse: 0.1496 - val_classification_AUC: 0.2232 - val_classification_f1_score: 0.5410 - val_classification_loss: -2.8352 - val_classification_precision: 0.9354 - val_classification_recall: 0.9354 - val_loss: 6.0094 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - bounding_box_loss: 0.5291 - bounding_box_mse: 0.5258 - classification_AUC: 0.3917 - classification_f1_score: 0.6122 - classification_loss: -8.0260 - classification_precision: 0.9465 - classification_recall: 0.9943 - loss: 1.3089\n",
      "Epoch 14: val_loss improved from 6.00939 to 5.24223, saving model to output/checkpoints/ckpt_14.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 338ms/step - bounding_box_loss: 0.5341 - bounding_box_mse: 0.5284 - classification_AUC: 0.3932 - classification_f1_score: 0.6098 - classification_loss: -8.0710 - classification_precision: 0.9489 - classification_recall: 0.9941 - loss: 1.3089 - val_bounding_box_loss: 0.1395 - val_bounding_box_mse: 0.1419 - val_classification_AUC: 0.2287 - val_classification_f1_score: 0.5410 - val_classification_loss: -3.4497 - val_classification_precision: 0.9296 - val_classification_recall: 0.9544 - val_loss: 5.2422 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - bounding_box_loss: 0.6117 - bounding_box_mse: 0.6070 - classification_AUC: 0.3448 - classification_f1_score: 0.6387 - classification_loss: -10.9480 - classification_precision: 0.9849 - classification_recall: 0.9944 - loss: -1.6479\n",
      "Epoch 15: val_loss improved from 5.24223 to 3.93164, saving model to output/checkpoints/ckpt_15.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 335ms/step - bounding_box_loss: 0.6195 - bounding_box_mse: 0.6114 - classification_AUC: 0.3509 - classification_f1_score: 0.6384 - classification_loss: -10.9616 - classification_precision: 0.9844 - classification_recall: 0.9946 - loss: -1.6504 - val_bounding_box_loss: 0.1328 - val_bounding_box_mse: 0.1333 - val_classification_AUC: 0.2234 - val_classification_f1_score: 0.5474 - val_classification_loss: -5.1448 - val_classification_precision: 0.9294 - val_classification_recall: 0.9506 - val_loss: 3.9316 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - bounding_box_loss: 0.8285 - bounding_box_mse: 0.8108 - classification_AUC: 0.3693 - classification_f1_score: 0.6235 - classification_loss: -13.2638 - classification_precision: 0.9914 - classification_recall: 0.9913 - loss: -3.7516\n",
      "Epoch 16: val_loss improved from 3.93164 to 3.31691, saving model to output/checkpoints/ckpt_16.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 370ms/step - bounding_box_loss: 0.8511 - bounding_box_mse: 0.8202 - classification_AUC: 0.3731 - classification_f1_score: 0.6293 - classification_loss: -13.5665 - classification_precision: 0.9901 - classification_recall: 0.9919 - loss: -3.9915 - val_bounding_box_loss: 0.1311 - val_bounding_box_mse: 0.1329 - val_classification_AUC: 0.2253 - val_classification_f1_score: 0.5474 - val_classification_loss: -5.5603 - val_classification_precision: 0.9257 - val_classification_recall: 0.9468 - val_loss: 3.3169 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - bounding_box_loss: 1.1944 - bounding_box_mse: 1.1991 - classification_AUC: 0.3836 - classification_f1_score: 0.6614 - classification_loss: -18.2878 - classification_precision: 0.9944 - classification_recall: 0.9978 - loss: -8.8068\n",
      "Epoch 17: val_loss did not improve from 3.31691\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - bounding_box_loss: 1.1807 - bounding_box_mse: 1.1889 - classification_AUC: 0.3864 - classification_f1_score: 0.6635 - classification_loss: -18.1517 - classification_precision: 0.9941 - classification_recall: 0.9961 - loss: -8.9110 - val_bounding_box_loss: 0.2005 - val_bounding_box_mse: 0.1987 - val_classification_AUC: 0.1908 - val_classification_f1_score: 0.5474 - val_classification_loss: -5.6875 - val_classification_precision: 0.9269 - val_classification_recall: 0.9163 - val_loss: 3.7960 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - bounding_box_loss: 1.3291 - bounding_box_mse: 1.3319 - classification_AUC: 0.4220 - classification_f1_score: 0.7288 - classification_loss: -27.0362 - classification_precision: 0.9909 - classification_recall: 0.9989 - loss: -17.2086\n",
      "Epoch 18: val_loss improved from 3.31691 to 0.95361, saving model to output/checkpoints/ckpt_18.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 344ms/step - bounding_box_loss: 1.3334 - bounding_box_mse: 1.3383 - classification_AUC: 0.4216 - classification_f1_score: 0.7266 - classification_loss: -26.8165 - classification_precision: 0.9915 - classification_recall: 0.9985 - loss: -17.0308 - val_bounding_box_loss: 0.1579 - val_bounding_box_mse: 0.1598 - val_classification_AUC: 0.2026 - val_classification_f1_score: 0.5886 - val_classification_loss: -8.4289 - val_classification_precision: 0.9176 - val_classification_recall: 0.9734 - val_loss: 0.9536 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - bounding_box_loss: 2.0347 - bounding_box_mse: 2.0490 - classification_AUC: 0.3879 - classification_f1_score: 0.6937 - classification_loss: -28.1348 - classification_precision: 0.9929 - classification_recall: 0.9971 - loss: -18.0002\n",
      "Epoch 19: val_loss improved from 0.95361 to -3.42542, saving model to output/checkpoints/ckpt_19.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 359ms/step - bounding_box_loss: 1.9906 - bounding_box_mse: 2.0156 - classification_AUC: 0.3933 - classification_f1_score: 0.6939 - classification_loss: -27.8214 - classification_precision: 0.9913 - classification_recall: 0.9960 - loss: -18.0626 - val_bounding_box_loss: 0.2404 - val_bounding_box_mse: 0.2493 - val_classification_AUC: 0.1978 - val_classification_f1_score: 0.6857 - val_classification_loss: -13.1754 - val_classification_precision: 0.9237 - val_classification_recall: 0.9202 - val_loss: -3.4254 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - bounding_box_loss: 2.2880 - bounding_box_mse: 2.3154 - classification_AUC: 0.3787 - classification_f1_score: 0.7396 - classification_loss: -36.2916 - classification_precision: 0.9982 - classification_recall: 0.9977 - loss: -25.7621\n",
      "Epoch 20: val_loss improved from -3.42542 to -5.42343, saving model to output/checkpoints/ckpt_20.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 336ms/step - bounding_box_loss: 2.2451 - bounding_box_mse: 2.2930 - classification_AUC: 0.3859 - classification_f1_score: 0.7371 - classification_loss: -35.9517 - classification_precision: 0.9979 - classification_recall: 0.9971 - loss: -25.6819 - val_bounding_box_loss: 0.4417 - val_bounding_box_mse: 0.4488 - val_classification_AUC: 0.2006 - val_classification_f1_score: 0.6613 - val_classification_loss: -16.4758 - val_classification_precision: 0.9429 - val_classification_recall: 0.8783 - val_loss: -5.4234 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - bounding_box_loss: 2.2489 - bounding_box_mse: 2.2443 - classification_AUC: 0.3629 - classification_f1_score: 0.6647 - classification_loss: -34.5604 - classification_precision: 0.9907 - classification_recall: 0.9925 - loss: -23.8284\n",
      "Epoch 21: val_loss improved from -5.42343 to -7.58545, saving model to output/checkpoints/ckpt_21.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 396ms/step - bounding_box_loss: 2.2620 - bounding_box_mse: 2.2540 - classification_AUC: 0.3733 - classification_f1_score: 0.6710 - classification_loss: -35.1277 - classification_precision: 0.9894 - classification_recall: 0.9925 - loss: -24.4142 - val_bounding_box_loss: 0.3084 - val_bounding_box_mse: 0.3152 - val_classification_AUC: 0.2239 - val_classification_f1_score: 0.6047 - val_classification_loss: -17.5789 - val_classification_precision: 0.9414 - val_classification_recall: 0.9163 - val_loss: -7.5854 - learning_rate: 1.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - bounding_box_loss: 3.1952 - bounding_box_mse: 3.1710 - classification_AUC: 0.3934 - classification_f1_score: 0.7198 - classification_loss: -46.0095 - classification_precision: 0.9874 - classification_recall: 0.9912 - loss: -34.3264\n",
      "Epoch 22: val_loss did not improve from -7.58545\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - bounding_box_loss: 3.2387 - bounding_box_mse: 3.1964 - classification_AUC: 0.4000 - classification_f1_score: 0.7228 - classification_loss: -46.5188 - classification_precision: 0.9870 - classification_recall: 0.9918 - loss: -34.8210 - val_bounding_box_loss: 0.3054 - val_bounding_box_mse: 0.3137 - val_classification_AUC: 0.2050 - val_classification_f1_score: 0.5716 - val_classification_loss: -12.5732 - val_classification_precision: 0.9158 - val_classification_recall: 0.9506 - val_loss: -2.9927 - learning_rate: 1.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - bounding_box_loss: 3.5902 - bounding_box_mse: 3.6101 - classification_AUC: 0.4145 - classification_f1_score: 0.7022 - classification_loss: -54.2949 - classification_precision: 0.9826 - classification_recall: 1.0000 - loss: -41.8961\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 23: val_loss did not improve from -7.58545\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - bounding_box_loss: 3.6096 - bounding_box_mse: 3.6445 - classification_AUC: 0.4216 - classification_f1_score: 0.7034 - classification_loss: -54.8677 - classification_precision: 0.9828 - classification_recall: 1.0000 - loss: -42.2447 - val_bounding_box_loss: 0.1515 - val_bounding_box_mse: 0.1581 - val_classification_AUC: 0.2117 - val_classification_f1_score: 0.5657 - val_classification_loss: -12.2420 - val_classification_precision: 0.9143 - val_classification_recall: 0.9734 - val_loss: -3.9519 - learning_rate: 1.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - bounding_box_loss: 4.0089 - bounding_box_mse: 3.9988 - classification_AUC: 0.4834 - classification_f1_score: 0.7270 - classification_loss: -66.2006 - classification_precision: 0.9860 - classification_recall: 1.0000 - loss: -53.7961\n",
      "Epoch 24: val_loss did not improve from -7.58545\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - bounding_box_loss: 3.9955 - bounding_box_mse: 3.9779 - classification_AUC: 0.4824 - classification_f1_score: 0.7270 - classification_loss: -65.6474 - classification_precision: 0.9864 - classification_recall: 1.0000 - loss: -53.3706 - val_bounding_box_loss: 0.1317 - val_bounding_box_mse: 0.1365 - val_classification_AUC: 0.2101 - val_classification_f1_score: 0.5716 - val_classification_loss: -13.1633 - val_classification_precision: 0.9209 - val_classification_recall: 0.9734 - val_loss: -4.8714 - learning_rate: 1.0000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - bounding_box_loss: 3.3924 - bounding_box_mse: 3.3882 - classification_AUC: 0.4389 - classification_f1_score: 0.7598 - classification_loss: -64.3548 - classification_precision: 0.9677 - classification_recall: 0.9973 - loss: -52.3869\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\n",
      "Epoch 25: val_loss did not improve from -7.58545\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - bounding_box_loss: 3.3928 - bounding_box_mse: 3.3853 - classification_AUC: 0.4424 - classification_f1_score: 0.7552 - classification_loss: -64.0931 - classification_precision: 0.9694 - classification_recall: 0.9972 - loss: -52.1058 - val_bounding_box_loss: 0.1168 - val_bounding_box_mse: 0.1198 - val_classification_AUC: 0.2063 - val_classification_f1_score: 0.5886 - val_classification_loss: -14.1554 - val_classification_precision: 0.9200 - val_classification_recall: 0.9620 - val_loss: -5.7852 - learning_rate: 1.0000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - bounding_box_loss: 3.1714 - bounding_box_mse: 3.1954 - classification_AUC: 0.3709 - classification_f1_score: 0.7387 - classification_loss: -63.2772 - classification_precision: 0.9952 - classification_recall: 0.9918 - loss: -51.6574\n",
      "Epoch 26: val_loss did not improve from -7.58545\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - bounding_box_loss: 3.1647 - bounding_box_mse: 3.2067 - classification_AUC: 0.3830 - classification_f1_score: 0.7381 - classification_loss: -63.4185 - classification_precision: 0.9944 - classification_recall: 0.9919 - loss: -51.8828 - val_bounding_box_loss: 0.1160 - val_bounding_box_mse: 0.1191 - val_classification_AUC: 0.2063 - val_classification_f1_score: 0.5886 - val_classification_loss: -14.6853 - val_classification_precision: 0.9200 - val_classification_recall: 0.9620 - val_loss: -6.2829 - learning_rate: 1.0000e-06\n",
      "Epoch 27/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - bounding_box_loss: 4.2355 - bounding_box_mse: 4.2551 - classification_AUC: 0.3916 - classification_f1_score: 0.6788 - classification_loss: -61.3490 - classification_precision: 0.9885 - classification_recall: 0.9946 - loss: -48.5253\n",
      "Epoch 27: val_loss did not improve from -7.58545\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - bounding_box_loss: 4.1712 - bounding_box_mse: 4.2056 - classification_AUC: 0.4023 - classification_f1_score: 0.6853 - classification_loss: -61.8516 - classification_precision: 0.9876 - classification_recall: 0.9948 - loss: -49.0648 - val_bounding_box_loss: 0.1160 - val_bounding_box_mse: 0.1191 - val_classification_AUC: 0.2061 - val_classification_f1_score: 0.5886 - val_classification_loss: -14.9852 - val_classification_precision: 0.9200 - val_classification_recall: 0.9620 - val_loss: -6.5648 - learning_rate: 1.0000e-06\n",
      "Epoch 28/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - bounding_box_loss: 3.3747 - bounding_box_mse: 3.3863 - classification_AUC: 0.4755 - classification_f1_score: 0.7278 - classification_loss: -66.7323 - classification_precision: 0.9878 - classification_recall: 0.9945 - loss: -54.8208\n",
      "Epoch 28: val_loss did not improve from -7.58545\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - bounding_box_loss: 3.3621 - bounding_box_mse: 3.3823 - classification_AUC: 0.4760 - classification_f1_score: 0.7285 - classification_loss: -66.5796 - classification_precision: 0.9879 - classification_recall: 0.9947 - loss: -54.6920 - val_bounding_box_loss: 0.1161 - val_bounding_box_mse: 0.1192 - val_classification_AUC: 0.2056 - val_classification_f1_score: 0.5886 - val_classification_loss: -15.5062 - val_classification_precision: 0.9231 - val_classification_recall: 0.9582 - val_loss: -7.0416 - learning_rate: 1.0000e-06\n",
      "Epoch 29/50\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - bounding_box_loss: 3.0581 - bounding_box_mse: 3.0581 - classification_AUC: 0.4437 - classification_f1_score: 0.7196 - classification_loss: -65.1649 - classification_precision: 0.9914 - classification_recall: 1.0000 - loss: -53.5547\n",
      "Epoch 29: val_loss did not improve from -7.58545\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - bounding_box_loss: 3.0599 - bounding_box_mse: 3.0650 - classification_AUC: 0.4479 - classification_f1_score: 0.7195 - classification_loss: -65.0587 - classification_precision: 0.9898 - classification_recall: 1.0000 - loss: -53.4457 - val_bounding_box_loss: 0.1154 - val_bounding_box_mse: 0.1185 - val_classification_AUC: 0.2058 - val_classification_f1_score: 0.5886 - val_classification_loss: -15.9200 - val_classification_precision: 0.9231 - val_classification_recall: 0.9582 - val_loss: -7.4319 - learning_rate: 1.0000e-06\n",
      "Epoch 30/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - bounding_box_loss: 3.4669 - bounding_box_mse: 3.4656 - classification_AUC: 0.4747 - classification_f1_score: 0.7105 - classification_loss: -69.7098 - classification_precision: 0.9711 - classification_recall: 0.9935 - loss: -57.7662\n",
      "Epoch 30: val_loss improved from -7.58545 to -8.12360, saving model to output/checkpoints/ckpt_30.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 344ms/step - bounding_box_loss: 3.4843 - bounding_box_mse: 3.4819 - classification_AUC: 0.4753 - classification_f1_score: 0.7125 - classification_loss: -69.3142 - classification_precision: 0.9714 - classification_recall: 0.9933 - loss: -57.4098 - val_bounding_box_loss: 0.1175 - val_bounding_box_mse: 0.1208 - val_classification_AUC: 0.2052 - val_classification_f1_score: 0.6047 - val_classification_loss: -16.7313 - val_classification_precision: 0.9265 - val_classification_recall: 0.9582 - val_loss: -8.1236 - learning_rate: 1.0000e-06\n",
      "Epoch 31/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - bounding_box_loss: 2.9089 - bounding_box_mse: 2.8990 - classification_AUC: 0.3638 - classification_f1_score: 0.6581 - classification_loss: -56.9740 - classification_precision: 0.9934 - classification_recall: 0.9744 - loss: -45.5071\n",
      "Epoch 31: val_loss improved from -8.12360 to -8.41851, saving model to output/checkpoints/ckpt_31.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 347ms/step - bounding_box_loss: 2.9318 - bounding_box_mse: 2.9144 - classification_AUC: 0.3762 - classification_f1_score: 0.6680 - classification_loss: -58.0483 - classification_precision: 0.9918 - classification_recall: 0.9762 - loss: -46.5544 - val_bounding_box_loss: 0.1172 - val_bounding_box_mse: 0.1206 - val_classification_AUC: 0.2053 - val_classification_f1_score: 0.6047 - val_classification_loss: -17.0614 - val_classification_precision: 0.9299 - val_classification_recall: 0.9582 - val_loss: -8.4185 - learning_rate: 1.0000e-06\n",
      "Epoch 32/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - bounding_box_loss: 3.5438 - bounding_box_mse: 3.5385 - classification_AUC: 0.4458 - classification_f1_score: 0.7085 - classification_loss: -65.8998 - classification_precision: 0.9762 - classification_recall: 0.9983 - loss: -53.8847\n",
      "Epoch 32: val_loss improved from -8.41851 to -9.05104, saving model to output/checkpoints/ckpt_32.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 340ms/step - bounding_box_loss: 3.5291 - bounding_box_mse: 3.5197 - classification_AUC: 0.4467 - classification_f1_score: 0.7094 - classification_loss: -65.5342 - classification_precision: 0.9754 - classification_recall: 0.9976 - loss: -53.5949 - val_bounding_box_loss: 0.1177 - val_bounding_box_mse: 0.1211 - val_classification_AUC: 0.2038 - val_classification_f1_score: 0.6047 - val_classification_loss: -17.7545 - val_classification_precision: 0.9294 - val_classification_recall: 0.9506 - val_loss: -9.0510 - learning_rate: 1.0000e-06\n",
      "Epoch 33/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - bounding_box_loss: 3.7225 - bounding_box_mse: 3.7291 - classification_AUC: 0.4024 - classification_f1_score: 0.6617 - classification_loss: -60.3535 - classification_precision: 0.9862 - classification_recall: 0.9914 - loss: -47.8521\n",
      "Epoch 33: val_loss improved from -9.05104 to -9.29815, saving model to output/checkpoints/ckpt_33.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 357ms/step - bounding_box_loss: 3.7001 - bounding_box_mse: 3.7117 - classification_AUC: 0.4098 - classification_f1_score: 0.6689 - classification_loss: -60.9459 - classification_precision: 0.9855 - classification_recall: 0.9920 - loss: -48.2972 - val_bounding_box_loss: 0.1169 - val_bounding_box_mse: 0.1204 - val_classification_AUC: 0.2037 - val_classification_f1_score: 0.6047 - val_classification_loss: -18.0273 - val_classification_precision: 0.9291 - val_classification_recall: 0.9468 - val_loss: -9.2981 - learning_rate: 1.0000e-06\n",
      "Epoch 34/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - bounding_box_loss: 3.1932 - bounding_box_mse: 3.1903 - classification_AUC: 0.3912 - classification_f1_score: 0.7723 - classification_loss: -66.0417 - classification_precision: 0.9872 - classification_recall: 0.9994 - loss: -55.3164\n",
      "Epoch 34: val_loss improved from -9.29815 to -9.82346, saving model to output/checkpoints/ckpt_34.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 347ms/step - bounding_box_loss: 3.1925 - bounding_box_mse: 3.1875 - classification_AUC: 0.4014 - classification_f1_score: 0.7675 - classification_loss: -64.9699 - classification_precision: 0.9873 - classification_recall: 0.9990 - loss: -55.0108 - val_bounding_box_loss: 0.1178 - val_bounding_box_mse: 0.1214 - val_classification_AUC: 0.2033 - val_classification_f1_score: 0.6098 - val_classification_loss: -18.6050 - val_classification_precision: 0.9283 - val_classification_recall: 0.9354 - val_loss: -9.8235 - learning_rate: 1.0000e-06\n",
      "Epoch 35/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - bounding_box_loss: 3.0468 - bounding_box_mse: 3.0491 - classification_AUC: 0.4366 - classification_f1_score: 0.7264 - classification_loss: -66.3415 - classification_precision: 0.9918 - classification_recall: 1.0000 - loss: -54.5603\n",
      "Epoch 35: val_loss improved from -9.82346 to -9.88765, saving model to output/checkpoints/ckpt_35.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 340ms/step - bounding_box_loss: 3.0402 - bounding_box_mse: 3.0442 - classification_AUC: 0.4403 - classification_f1_score: 0.7273 - classification_loss: -66.4844 - classification_precision: 0.9919 - classification_recall: 1.0000 - loss: -54.5738 - val_bounding_box_loss: 0.1168 - val_bounding_box_mse: 0.1202 - val_classification_AUC: 0.2031 - val_classification_f1_score: 0.6098 - val_classification_loss: -18.6767 - val_classification_precision: 0.9283 - val_classification_recall: 0.9354 - val_loss: -9.8877 - learning_rate: 1.0000e-06\n",
      "Epoch 36/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - bounding_box_loss: 3.2827 - bounding_box_mse: 3.2913 - classification_AUC: 0.4609 - classification_f1_score: 0.7069 - classification_loss: -65.4786 - classification_precision: 0.9918 - classification_recall: 1.0000 - loss: -53.7185\n",
      "Epoch 36: val_loss improved from -9.88765 to -10.29811, saving model to output/checkpoints/ckpt_36.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 355ms/step - bounding_box_loss: 3.2615 - bounding_box_mse: 3.2765 - classification_AUC: 0.4627 - classification_f1_score: 0.7099 - classification_loss: -65.4503 - classification_precision: 0.9919 - classification_recall: 1.0000 - loss: -53.7683 - val_bounding_box_loss: 0.1169 - val_bounding_box_mse: 0.1203 - val_classification_AUC: 0.2034 - val_classification_f1_score: 0.6098 - val_classification_loss: -19.1458 - val_classification_precision: 0.9280 - val_classification_recall: 0.9316 - val_loss: -10.2981 - learning_rate: 1.0000e-06\n",
      "Epoch 37/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - bounding_box_loss: 3.5254 - bounding_box_mse: 3.4632 - classification_AUC: 0.4490 - classification_f1_score: 0.7063 - classification_loss: -63.9533 - classification_precision: 0.9858 - classification_recall: 1.0000 - loss: -51.9354\n",
      "Epoch 37: val_loss improved from -10.29811 to -10.46491, saving model to output/checkpoints/ckpt_37.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 333ms/step - bounding_box_loss: 3.5729 - bounding_box_mse: 3.4640 - classification_AUC: 0.4527 - classification_f1_score: 0.7098 - classification_loss: -64.2353 - classification_precision: 0.9866 - classification_recall: 1.0000 - loss: -52.2156 - val_bounding_box_loss: 0.1169 - val_bounding_box_mse: 0.1203 - val_classification_AUC: 0.2028 - val_classification_f1_score: 0.6098 - val_classification_loss: -19.3447 - val_classification_precision: 0.9280 - val_classification_recall: 0.9316 - val_loss: -10.4649 - learning_rate: 1.0000e-06\n",
      "Epoch 38/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - bounding_box_loss: 3.1901 - bounding_box_mse: 3.2118 - classification_AUC: 0.3701 - classification_f1_score: 0.7326 - classification_loss: -63.2951 - classification_precision: 0.9989 - classification_recall: 0.9910 - loss: -51.2474\n",
      "Epoch 38: val_loss improved from -10.46491 to -10.46937, saving model to output/checkpoints/ckpt_38.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 356ms/step - bounding_box_loss: 3.1806 - bounding_box_mse: 3.2187 - classification_AUC: 0.3819 - classification_f1_score: 0.7328 - classification_loss: -63.8496 - classification_precision: 0.9985 - classification_recall: 0.9917 - loss: -51.5833 - val_bounding_box_loss: 0.1158 - val_bounding_box_mse: 0.1191 - val_classification_AUC: 0.2031 - val_classification_f1_score: 0.6098 - val_classification_loss: -19.3602 - val_classification_precision: 0.9280 - val_classification_recall: 0.9316 - val_loss: -10.4694 - learning_rate: 1.0000e-06\n",
      "Epoch 39/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - bounding_box_loss: 2.8385 - bounding_box_mse: 2.8405 - classification_AUC: 0.4626 - classification_f1_score: 0.7045 - classification_loss: -66.1884 - classification_precision: 0.9826 - classification_recall: 1.0000 - loss: -54.5180\n",
      "Epoch 39: val_loss improved from -10.46937 to -10.57994, saving model to output/checkpoints/ckpt_39.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 331ms/step - bounding_box_loss: 2.8466 - bounding_box_mse: 2.8500 - classification_AUC: 0.4639 - classification_f1_score: 0.7073 - classification_loss: -66.4385 - classification_precision: 0.9824 - classification_recall: 1.0000 - loss: -54.5514 - val_bounding_box_loss: 0.1158 - val_bounding_box_mse: 0.1191 - val_classification_AUC: 0.2028 - val_classification_f1_score: 0.6098 - val_classification_loss: -19.5009 - val_classification_precision: 0.9280 - val_classification_recall: 0.9316 - val_loss: -10.5799 - learning_rate: 1.0000e-06\n",
      "Epoch 40/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - bounding_box_loss: 3.1523 - bounding_box_mse: 3.1478 - classification_AUC: 0.3732 - classification_f1_score: 0.6783 - classification_loss: -53.2466 - classification_precision: 0.9906 - classification_recall: 0.9742 - loss: -41.4691\n",
      "Epoch 40: val_loss improved from -10.57994 to -10.75297, saving model to output/checkpoints/ckpt_40.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 349ms/step - bounding_box_loss: 3.1356 - bounding_box_mse: 3.1277 - classification_AUC: 0.3832 - classification_f1_score: 0.6848 - classification_loss: -54.3649 - classification_precision: 0.9903 - classification_recall: 0.9760 - loss: -42.5504 - val_bounding_box_loss: 0.1157 - val_bounding_box_mse: 0.1189 - val_classification_AUC: 0.2033 - val_classification_f1_score: 0.6098 - val_classification_loss: -19.7040 - val_classification_precision: 0.9313 - val_classification_recall: 0.9278 - val_loss: -10.7530 - learning_rate: 1.0000e-06\n",
      "Epoch 41/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - bounding_box_loss: 3.5155 - bounding_box_mse: 3.5390 - classification_AUC: 0.4171 - classification_f1_score: 0.7382 - classification_loss: -68.1772 - classification_precision: 0.9924 - classification_recall: 1.0000 - loss: -56.1045\n",
      "Epoch 41: val_loss improved from -10.75297 to -10.82831, saving model to output/checkpoints/ckpt_41.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 341ms/step - bounding_box_loss: 3.4898 - bounding_box_mse: 3.5309 - classification_AUC: 0.4251 - classification_f1_score: 0.7372 - classification_loss: -68.1367 - classification_precision: 0.9919 - classification_recall: 1.0000 - loss: -56.0871 - val_bounding_box_loss: 0.1161 - val_bounding_box_mse: 0.1193 - val_classification_AUC: 0.2028 - val_classification_f1_score: 0.6098 - val_classification_loss: -19.8051 - val_classification_precision: 0.9310 - val_classification_recall: 0.9240 - val_loss: -10.8283 - learning_rate: 1.0000e-06\n",
      "Epoch 42/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - bounding_box_loss: 3.2212 - bounding_box_mse: 3.2444 - classification_AUC: 0.4793 - classification_f1_score: 0.7227 - classification_loss: -69.1464 - classification_precision: 0.9786 - classification_recall: 1.0000 - loss: -57.4518\n",
      "Epoch 42: val_loss improved from -10.82831 to -10.96661, saving model to output/checkpoints/ckpt_42.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 336ms/step - bounding_box_loss: 3.1939 - bounding_box_mse: 3.2345 - classification_AUC: 0.4780 - classification_f1_score: 0.7232 - classification_loss: -68.5785 - classification_precision: 0.9798 - classification_recall: 1.0000 - loss: -56.9717 - val_bounding_box_loss: 0.1171 - val_bounding_box_mse: 0.1204 - val_classification_AUC: 0.2022 - val_classification_f1_score: 0.6098 - val_classification_loss: -19.9758 - val_classification_precision: 0.9310 - val_classification_recall: 0.9240 - val_loss: -10.9666 - learning_rate: 1.0000e-06\n",
      "Epoch 43/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - bounding_box_loss: 3.2898 - bounding_box_mse: 3.2713 - classification_AUC: 0.3760 - classification_f1_score: 0.7273 - classification_loss: -65.5005 - classification_precision: 0.9951 - classification_recall: 1.0000 - loss: -53.8644\n",
      "Epoch 43: val_loss improved from -10.96661 to -11.24409, saving model to output/checkpoints/ckpt_43.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 371ms/step - bounding_box_loss: 3.2963 - bounding_box_mse: 3.2640 - classification_AUC: 0.3894 - classification_f1_score: 0.7272 - classification_loss: -65.5327 - classification_precision: 0.9947 - classification_recall: 1.0000 - loss: -54.0458 - val_bounding_box_loss: 0.1185 - val_bounding_box_mse: 0.1219 - val_classification_AUC: 0.2021 - val_classification_f1_score: 0.6047 - val_classification_loss: -20.3118 - val_classification_precision: 0.9308 - val_classification_recall: 0.9202 - val_loss: -11.2441 - learning_rate: 1.0000e-06\n",
      "Epoch 44/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - bounding_box_loss: 3.0884 - bounding_box_mse: 3.0736 - classification_AUC: 0.4116 - classification_f1_score: 0.6820 - classification_loss: -61.6217 - classification_precision: 0.9866 - classification_recall: 1.0000 - loss: -49.9489\n",
      "Epoch 44: val_loss did not improve from -11.24409\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - bounding_box_loss: 3.0968 - bounding_box_mse: 3.0708 - classification_AUC: 0.4196 - classification_f1_score: 0.6885 - classification_loss: -62.4413 - classification_precision: 0.9859 - classification_recall: 1.0000 - loss: -50.7376 - val_bounding_box_loss: 0.1192 - val_bounding_box_mse: 0.1227 - val_classification_AUC: 0.2013 - val_classification_f1_score: 0.6047 - val_classification_loss: -20.2506 - val_classification_precision: 0.9308 - val_classification_recall: 0.9202 - val_loss: -11.1792 - learning_rate: 1.0000e-06\n",
      "Epoch 45/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - bounding_box_loss: 3.5248 - bounding_box_mse: 3.5480 - classification_AUC: 0.3889 - classification_f1_score: 0.7477 - classification_loss: -64.6045 - classification_precision: 0.9874 - classification_recall: 0.9968 - loss: -53.5129\n",
      "Epoch 45: val_loss improved from -11.24409 to -11.61826, saving model to output/checkpoints/ckpt_45.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 340ms/step - bounding_box_loss: 3.4621 - bounding_box_mse: 3.5027 - classification_AUC: 0.3983 - classification_f1_score: 0.7468 - classification_loss: -63.8126 - classification_precision: 0.9875 - classification_recall: 0.9962 - loss: -53.5244 - val_bounding_box_loss: 0.1215 - val_bounding_box_mse: 0.1250 - val_classification_AUC: 0.2006 - val_classification_f1_score: 0.6098 - val_classification_loss: -20.7669 - val_classification_precision: 0.9308 - val_classification_recall: 0.9202 - val_loss: -11.6183 - learning_rate: 1.0000e-06\n",
      "Epoch 46/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - bounding_box_loss: 3.5137 - bounding_box_mse: 3.5176 - classification_AUC: 0.4560 - classification_f1_score: 0.7595 - classification_loss: -70.7192 - classification_precision: 0.9888 - classification_recall: 1.0000 - loss: -58.8058\n",
      "Epoch 46: val_loss improved from -11.61826 to -11.87341, saving model to output/checkpoints/ckpt_46.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 336ms/step - bounding_box_loss: 3.5054 - bounding_box_mse: 3.5122 - classification_AUC: 0.4581 - classification_f1_score: 0.7567 - classification_loss: -70.2221 - classification_precision: 0.9893 - classification_recall: 1.0000 - loss: -58.4329 - val_bounding_box_loss: 0.1227 - val_bounding_box_mse: 0.1263 - val_classification_AUC: 0.2009 - val_classification_f1_score: 0.6098 - val_classification_loss: -21.0521 - val_classification_precision: 0.9302 - val_classification_recall: 0.9125 - val_loss: -11.8734 - learning_rate: 1.0000e-06\n",
      "Epoch 47/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - bounding_box_loss: 3.2419 - bounding_box_mse: 3.2573 - classification_AUC: 0.4449 - classification_f1_score: 0.7311 - classification_loss: -66.4289 - classification_precision: 0.9897 - classification_recall: 0.9964 - loss: -54.6098\n",
      "Epoch 47: val_loss improved from -11.87341 to -11.87767, saving model to output/checkpoints/ckpt_47.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 327ms/step - bounding_box_loss: 3.2420 - bounding_box_mse: 3.2689 - classification_AUC: 0.4503 - classification_f1_score: 0.7315 - classification_loss: -66.3275 - classification_precision: 0.9895 - classification_recall: 0.9958 - loss: -54.4913 - val_bounding_box_loss: 0.1240 - val_bounding_box_mse: 0.1277 - val_classification_AUC: 0.2003 - val_classification_f1_score: 0.6098 - val_classification_loss: -21.0732 - val_classification_precision: 0.9302 - val_classification_recall: 0.9125 - val_loss: -11.8777 - learning_rate: 1.0000e-06\n",
      "Epoch 48/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - bounding_box_loss: 3.0030 - bounding_box_mse: 3.0054 - classification_AUC: 0.4121 - classification_f1_score: 0.7447 - classification_loss: -66.1599 - classification_precision: 0.9753 - classification_recall: 1.0000 - loss: -54.5794\n",
      "Epoch 48: val_loss improved from -11.87767 to -11.90716, saving model to output/checkpoints/ckpt_48.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 328ms/step - bounding_box_loss: 3.0315 - bounding_box_mse: 3.0357 - classification_AUC: 0.4201 - classification_f1_score: 0.7425 - classification_loss: -66.2347 - classification_precision: 0.9760 - classification_recall: 1.0000 - loss: -54.6087 - val_bounding_box_loss: 0.1265 - val_bounding_box_mse: 0.1302 - val_classification_AUC: 0.1983 - val_classification_f1_score: 0.6098 - val_classification_loss: -21.1298 - val_classification_precision: 0.9302 - val_classification_recall: 0.9125 - val_loss: -11.9072 - learning_rate: 1.0000e-06\n",
      "Epoch 49/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - bounding_box_loss: 3.1313 - bounding_box_mse: 3.1323 - classification_AUC: 0.3977 - classification_f1_score: 0.6911 - classification_loss: -62.3569 - classification_precision: 0.9933 - classification_recall: 0.9913 - loss: -50.7230\n",
      "Epoch 49: val_loss improved from -11.90716 to -12.19264, saving model to output/checkpoints/ckpt_49.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 327ms/step - bounding_box_loss: 3.0850 - bounding_box_mse: 3.0867 - classification_AUC: 0.4083 - classification_f1_score: 0.6955 - classification_loss: -62.5859 - classification_precision: 0.9927 - classification_recall: 0.9919 - loss: -51.0376 - val_bounding_box_loss: 0.1293 - val_bounding_box_mse: 0.1331 - val_classification_AUC: 0.1993 - val_classification_f1_score: 0.6248 - val_classification_loss: -21.4557 - val_classification_precision: 0.9409 - val_classification_recall: 0.9087 - val_loss: -12.1926 - learning_rate: 1.0000e-06\n",
      "Epoch 50/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - bounding_box_loss: 3.4005 - bounding_box_mse: 3.3892 - classification_AUC: 0.4600 - classification_f1_score: 0.7084 - classification_loss: -67.7700 - classification_precision: 0.9854 - classification_recall: 0.9955 - loss: -55.4204\n",
      "Epoch 50: val_loss did not improve from -12.19264\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - bounding_box_loss: 3.4012 - bounding_box_mse: 3.3814 - classification_AUC: 0.4605 - classification_f1_score: 0.7112 - classification_loss: -67.6114 - classification_precision: 0.9839 - classification_recall: 0.9941 - loss: -54.9656 - val_bounding_box_loss: 0.1292 - val_bounding_box_mse: 0.1330 - val_classification_AUC: 0.1983 - val_classification_f1_score: 0.6098 - val_classification_loss: -21.2912 - val_classification_precision: 0.9409 - val_classification_recall: 0.9087 - val_loss: -12.0204 - learning_rate: 1.0000e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 13:04:10.052590: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-03-12 13:04:12.174976: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1709', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-03-12 13:04:12.376474: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1716', 212 bytes spill stores, 244 bytes spill loads\n",
      "\n",
      "2025-03-12 13:04:12.382741: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot', 124 bytes spill stores, 124 bytes spill loads\n",
      "\n",
      "2025-03-12 13:04:12.469967: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1716_0', 208 bytes spill stores, 392 bytes spill loads\n",
      "\n",
      "2025-03-12 13:04:12.508752: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1716', 404 bytes spill stores, 404 bytes spill loads\n",
      "\n",
      "2025-03-12 13:04:12.611691: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1716', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-03-12 13:04:12.619461: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot', 256 bytes spill stores, 256 bytes spill loads\n",
      "\n",
      "2025-03-12 13:04:12.936950: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1716', 428 bytes spill stores, 532 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/12 13:04:14 WARNING mlflow.models.signature: Failed to infer schema for outputs. Setting schema to `Schema([ColSpec(type=AnyType())]` as default. To see the full traceback, set logging level to DEBUG.\n",
      "2025/03/12 13:04:22 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(\"/brain-tumor-resnet50\")\n",
    "mlflow.tensorflow.autolog(log_models=True, log_datasets=False)\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[callbacks],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Datasets setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 100)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prepare_test_dataset = AnnotationProcessor(annotation_file= str(TEST_DIR/'_annotations.csv'))\n",
    "_class_map = {v: k for k, v in enumerate(CLASS_NAME)}\n",
    "test_image_paths, test_class_ids, test_bboxes = prepare_test_dataset.process_annotations(image_dir=TEST_DIR, class_id_map=_class_map)\n",
    "\n",
    "len(test_image_paths), len(test_class_ids), len(test_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_padded_class_ids, test_padded_bbx = pad_cls_id_bbx(test_class_ids, test_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datasets = tf.data.Dataset.from_tensor_slices((test_image_paths,\n",
    "                                               test_padded_class_ids,\n",
    "                                               test_padded_bbx))\n",
    "\n",
    "test_ds = test_datasets.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.repeat()\\\n",
    "                .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                . map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                .prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - bounding_box_loss: 0.1248 - bounding_box_mse: 0.1248 - classification_AUC: 0.2031 - classification_f1_score: 0.7333 - classification_loss: -33.2948 - classification_precision: 0.9459 - classification_recall: 0.8750 - loss: -24.6150\n",
      "Testing accuracy:  {'bounding_box_loss': 0.12481380999088287, 'bounding_box_mse': 0.12481380999088287, 'classification_AUC': 0.203125, 'classification_f1_score': 0.7333333492279053, 'classification_loss': -33.29476547241211, 'classification_precision': 0.9459459185600281, 'classification_recall': 0.875, 'loss': -24.615001678466797}\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_ds, return_dict=True, steps=1)\n",
    "print(\"Testing accuracy: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bounding_box_loss': 0.12481380999088287,\n",
       " 'bounding_box_mse': 0.12481380999088287,\n",
       " 'classification_AUC': 0.203125,\n",
       " 'classification_f1_score': 0.7333333492279053,\n",
       " 'classification_loss': -33.29476547241211,\n",
       " 'classification_precision': 0.9459459185600281,\n",
       " 'classification_recall': 0.875,\n",
       " 'loss': -24.615001678466797}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metric_name, title, ylim=1):\n",
    "  plt.title(title)\n",
    "  plt.ylim(0,ylim)\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.legend([metric_name, 'val_' + metric_name])\n",
    "  plt.plot(history.history[metric_name],color='blue',label=metric_name)\n",
    "  plt.plot(history.history['val_' + metric_name],color='green',label='val_' + metric_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrics(\"loss\", \"val_loss\", ylim=0.2)\n",
    "plot_metrics(\"classification_f1_score\", \"classification_f1_score\", ylim=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrics(\"classification_loss\", \"Classification Loss\")\n",
    "plot_metrics(\"bounding_box_loss\", \"Bounding Box Loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
